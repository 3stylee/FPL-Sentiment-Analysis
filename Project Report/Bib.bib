%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Lewis Watt at 2024-11-01 14:27:34 +0000 


%% Saved with string encoding Unicode (UTF-8) 
@misc{raghunandh, 
	title={Building an FPL captain classifier}, 
	howpublished={\url{https://medium.com/datacomics/building-an-fpl-captain-classifier-cf4ee343ebcc}}, 
	journal={Medium}, 
	publisher={DataComics}, 
	author={GS, Raghunandh}, 
	year={2018}, 
	month={Sep}} 

@misc{surprisingly-popular,
  title={The Wisdom of Even Wiser Crowds},
  howpublished={\url{https://www.wsj.com/articles/the-wisdom-of-even-wiser-crowds-1487265722?tesla=y\&mod=vocus}},
  journal={Wall Street Journal},
  author={Daniel Akst},
  year={2017},
  month={Feb}}

@book{wisdom-of-crowds,
  title={The Wisdom of Crowds},
  author={Surowiecki, J.},
  isbn={9780307275059},
  url={https://books.google.co.uk/books?id=hHUsHOHqVzEC},
  year={2005},
  publisher={Knopf Doubleday Publishing Group}
}

@book{politics,
  title={Politics},
  author={Aristotle},
  isbn={9783849692841},
  lccn={lc33000965},
  series={Loeb classical library},
  url={https://books.google.co.uk/books?id=a5y5DgAAQBAJ},
  year={1932},
  publisher={Heinemann}
}


@article{galton,
	abstract = {IN these democratic days, any investigation into the the trustworthiness and peculiarities of popular judgments is of interest. The material about to be discussed refers to a small matter, but is much to the point.},
	author = {GALTON, FRANCIS},
	date = {1907/03/01},
	date-added = {2024-11-01 11:58:11 +0000},
	date-modified = {2024-11-01 14:21:23 +0000},
	doi = {10.1038/075450a0},
	id = {GALTON1907},
	isbn = {1476-4687},
	journal = {Nature},
	number = {1949},
	pages = {450--451},
	title = {Vox Populi},
	url = {https://doi.org/10.1038/075450a0},
	volume = {75},
	year = {1907},
	bdsk-url-1 = {https://doi.org/10.1038/075450a0}}
	
@article{bonello,
  title={Multi-stream data analytics for enhanced performance prediction in fantasy football},
  author={Bonello, Nicholas and Beel, Joeran and Lawless, Seamus and Debattista, Jeremy},
  journal={arXiv preprint arXiv:1912.07441},
  year={2019}
}

@phdthesis{whittaker,
  title={A study of information behaviour in the Fantasy Premier League community},
  author={Whittaker, Daniel},
  year={2022},
  school={C}
}

@inproceedings{bhatt,
  title={Who should be the captain this week? leveraging inferred diversity-enhanced crowd wisdom for a fantasy premier league captain prediction},
  author={Bhatt, Shreyansh and Chen, Keke and Shalin, Valerie L and Sheth, Amit P and Minnery, Brandon},
  booktitle={Proceedings of the international AAAI conference on Web and Social Media},
  volume={13},
  pages={103--113},
  year={2019}
}

@mastersthesis{kristiansen,
  title={Developing a Forecast-Based Optimization Model for Fantasy Premier League},
  author={Kristiansen, Bj{\o}rn K{\aa}re and Gupta, Akash and Eilertsen, William},
  year={2018},
  school={NTNU}
}

@article{matthews, title={Competing with Humans at Fantasy Football: Team Formation in Large Partially-Observable Domains}, volume={26}, url={https://ojs.aaai.org/index.php/AAAI/article/view/8259}, DOI={10.1609/aaai.v26i1.8259}, abstractNote={ &lt;p&gt; We present the first real-world benchmark for sequentially-optimal team formation, working within the framework of a class of online football prediction games known as Fantasy Football. We model the problem as a Bayesian reinforcement learning one, where the action space is exponential in the number of players and where the decision maker’s beliefs are over multiple characteristics of each footballer. We then exploit domain knowledge to construct computationally tractable solution techniques in order to build a competitive automated Fantasy Football manager. Thus, we are able to establish the baseline performance in this domain, even without complete information on footballers’ performances (accessible to human managers), showing that our agent is able to rank at around the top percentile when pitched against 2.5M human players. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Matthews, Tim and Ramchurn, Sarvapali and Chalkiadakis, Georgios}, year={2021}, month={Sep.}, pages={1394-1400} }

@inproceedings{bangdiwala,
author = {Bangdiwala, Malhar and Choudhari, Rutvik and Hegde, Adwait and Salunke, Abhijeet},
year = {2022},
month = {10},
pages = {},
title = {Using ML Models to Predict Points in Fantasy Premier League},
doi = {10.1109/ASIANCON55314.2022.9909447}
}

@article{papageorgiou,
	author = {Papageorgiou, George and Sarlis, Vangelis and Tjortjis, Christos},
	date = {2024/03/19},
	date-added = {2024-11-05 12:13:02 +0000},
	date-modified = {2024-11-05 12:13:02 +0000},
	doi = {10.1007/s41060-024-00523-y},
	id = {Papageorgiou2024},
	isbn = {2364-4168},
	journal = {International Journal of Data Science and Analytics},
	title = {An innovative method for accurate NBA player performance forecasting and line-up optimization in daily fantasy sports},
	url = {https://doi.org/10.1007/s41060-024-00523-y},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s41060-024-00523-y}}
	
@INPROCEEDINGS{rajesh,
  author={Rajesh, Vimal and Arjun, P and Jagtap, Kunal Ravikumar and M, Suneera C and Prakash, Jay},
  booktitle={2022 19th International Joint Conference on Computer Science and Software Engineering (JCSSE)}, 
  title={Player Recommendation System for Fantasy Premier League using Machine Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  keywords={Measurement;Industries;TV;Statistical analysis;Fixtures;Machine learning;Media;Recommendation System;Fantasy Sports;Machine Learning;Statistics;Football},
  doi={10.1109/JCSSE54890.2022.9836260}}

@book{liu,
  title={Sentiment Analysis and Opinion Mining},
  author={Liu, B.},
  isbn={9781608458851},
  series={Synthesis Lectures on Human Language Technologies},
  url={https://books.google.co.uk/books?id=AZBfAQAAQBAJ},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}

@article{patel,
  title={Sentiment analysis of customer feedback and reviews for airline services using language representation model},
  author={Patel, Aksh and Oza, Parita and Agrawal, Smita},
  journal={Procedia Computer Science},
  volume={218},
  pages={2459--2467},
  year={2023},
  publisher={Elsevier}
}

@book{athenian-democracy,
  title={Athenian Democracy},
  author={Thorley, J.},
  isbn={9781134364596},
  series={Lancaster Pamphlets in Ancient History},
  url={https://books.google.co.uk/books?id=-ANoUXtMbIAC},
  year={2012},
  publisher={Taylor \& Francis}
}


@article{droba,
  title={Methods used for measuring public opinion},
  author={Droba, Daniel D},
  journal={American Journal of Sociology},
  volume={37},
  number={3},
  pages={410--423},
  year={1931},
  publisher={University of Chicago Press}
}

@article{mantyla,
title = {The evolution of sentiment analysis—A review of research topics, venues, and top cited papers},
journal = {Computer Science Review},
volume = {27},
pages = {16-32},
year = {2018},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574013717300606},
author = {Mika V. Mäntylä and Daniel Graziotin and Miikka Kuutila},
keywords = {Sentiment analysis, Opinion mining, Bibliometric study, Text mining, Literature review, Topic modeling, Latent Dirichlet Allocation, Qualitative analysis},
}

@misc{shah,
  title={Top 5 Techniques for Sentiment Analysis in Natural Language Processing},
  howpublished={\url{https://medium.com/illumination/top-5-techniques-for-sentiment-analysis-in-natural-language-processing-c07ba5b83f64}},
  journal={Mediuml},
  author={Syed Huma Shah},
  year={2023},
  month={Dec}}
  
@Article{liapis,
AUTHOR = {Liapis, Charalampos M. and Karanikola, Aikaterini and Kotsiantis, Sotiris},
TITLE = {Investigating Deep Stock Market Forecasting with Sentiment Analysis},
JOURNAL = {Entropy},
VOLUME = {25},
YEAR = {2023},
NUMBER = {2},
ARTICLE-NUMBER = {219},
URL = {https://www.mdpi.com/1099-4300/25/2/219},
PubMedID = {36832586},
ISSN = {1099-4300},
ABSTRACT = {When forecasting financial time series, incorporating relevant sentiment analysis data into the feature space is a common assumption to increase the capacities of the model. In addition, deep learning architectures and state-of-the-art schemes are increasingly used due to their efficiency. This work compares state-of-the-art methods in financial time series forecasting incorporating sentiment analysis. Through an extensive experimental process, 67 different feature setups consisting of stock closing prices and sentiment scores were tested on a variety of different datasets and metrics. In total, 30 state-of-the-art algorithmic schemes were used over two case studies: one comparing methods and one comparing input feature setups. The aggregated results indicate, on the one hand, the prevalence of a proposed method and, on the other, a conditional improvement in model efficiency after the incorporation of sentiment setups in certain forecast time frames.},
DOI = {10.3390/e25020219}
}

@article{taboada,
    author = {Taboada, Maite and Brooke, Julian and Tofiloski, Milan and Voll, Kimberly and Stede, Manfred},
    title = "{Lexicon-Based Methods for Sentiment Analysis}",
    journal = {Computational Linguistics},
    volume = {37},
    number = {2},
    pages = {267-307},
    year = {2011},
    month = {06},
    abstract = "{We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensification and negation. SO-CAL is applied to the polarity classification task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and in completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.}",
    issn = {0891-2017},
    doi = {10.1162/COLI_a_00049},
    url = {https://doi.org/10.1162/COLI\_a\_00049},
    eprint = {https://direct.mit.edu/coli/article-pdf/37/2/267/1798865/coli\_a\_00049.pdf},
}

@article{hamilton,
  author       = {William L. Hamilton and
                  Kevin Clark and
                  Jure Leskovec and
                  Dan Jurafsky},
  title        = {Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora},
  journal      = {CoRR},
  volume       = {abs/1606.02820},
  year         = {2016},
  url          = {http://arxiv.org/abs/1606.02820},
  eprinttype    = {arXiv},
  eprint       = {1606.02820},
  timestamp    = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HamiltonCLJ16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
  
@misc{fpl-origins,
  howpublished={\url{https://fpltips.com/the-fpl-era-the-history-of-fantasy-football/}},
  journal={fpltips.com},
  year={2023},
  month={June}}

@misc{fpl-prizes,
  howpublished={\url{https://fantasy.premierleague.com/prizes}},
  year={2024},
  month={November}}
  
 @misc{fpl-market-growth,
  howpublished={\url{https://www.skyquestt.com/report/fantasy-sports-market}},
  year={2024},
  month={April}}

@misc{fpl-playercount,
  howpublished={\url{https://www.attackingfootball.com/how-many-people-play-fantasy-premier-league-fpl/}},
  year={2023},
  month={November}}
  
@misc{ffh,
 howpublished={\url{https://www.fantasyfootballhub.co.uk/}},
 year={2024},
 month={November}}

@misc{ffh-users,
 howpublished={\url{https://thenextweb.com/news/ai-is-killing-fantasy-football-fpl}},
 year={2024},
 month={September}}
 
 @misc{r/FPL,
  howpublished={\url{https://www.reddit.com/r/FantasyPL/}},
  year={2024},
  month={November}}
  
 @misc{@OfficialFPL,
 howpublished={\url{https://x.com/officialfpl}},
 year={2024},
 month={November}}
 
 @misc{opta,
  howpublished={\url{https://optaplayerstats.statsperform.com/en_GB/soccer}},
  year={2024},
  month={November}}
 
@article{multilevel-models,
title = {A practical guide to multilevel modeling},
journal = {Journal of School Psychology},
volume = {48},
number = {1},
pages = {85-112},
year = {2010},
issn = {0022-4405},
doi = {https://doi.org/10.1016/j.jsp.2009.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0022440509000545},
author = {James L. Peugh},
keywords = {Multilevel modeling, Cross-sectional, Longitudinal, HLM, SPSS, SAS}}

@misc{BERT,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@article{elankath,
	author = {Syam Mohan Elankath and Sunitha Ramamirtham},
	title = {Sentiment analysis of Malayalam tweets using bidirectional encoder representations from transformers: a study},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	volume = {29},
	number = {3},
	year = {2023},
	keywords = {BERT; Deep learning; Machine learning; Malayalam tweets; Sentiment analysis},
	issn = {2502-4760},	
	pages = {1817--1826}	
	doi = {10.11591/ijeecs.v29.i3.pp1817-1826},
	url = {https://ijeecs.iaescore.com/index.php/IJEECS/article/view/28890}}
	
@inproceedings{hoang,
    title = "Aspect-Based Sentiment Analysis using {BERT}",
    author = "Hoang, Mickel  and
      Bihorac, Oskar Alija  and
      Rouces, Jacobo",
    editor = "Hartmann, Mareike  and
      Plank, Barbara",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://aclanthology.org/W19-6120",
    pages = "187--196",
    abstract = "Sentiment analysis has become very popular in both research and business due to the increasing amount of opinionated text from Internet users. Standard sentiment analysis deals with classifying the overall sentiment of a text, but this doesn{'}t include other important information such as towards which entity, topic or aspect within the text the sentiment is directed. Aspect-based sentiment analysis (ABSA) is a more complex task that consists in identifying both sentiments and aspects. This paper shows the potential of using the contextual word representations from the pre-trained language model BERT, together with a fine-tuning method with additional generated text, in order to solve out-of-domain ABSA and outperform previous state-of-the-art results on SemEval-2015 Task 12 subtask 2 and SemEval-2016 Task 5. To the best of our knowledge, no other existing work has been done on out-of-domain ABSA for aspect classification.",
}

@misc{fine-tuning,
  title={What is fine-tuning?},
  howpublished={\url{https://www.ibm.com/topics/fine-tuning}},
  author={Dave Bergmann},
  month={March},
  year={2024}
}

@misc{chatGPT,
  title={ChatGPT Architecture},
  howpublished={\url{https://medium.com/@ashish.sharma1981/chatgpt-architecture-exploring-the-inner-workings-of-the-language-model-41731fc05483}},
  year={2023},
  month={October}}

@misc{attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@inproceedings{transformer-applications,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",
}

@misc{word-piece,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1609.08144}, 
}

@misc{embedding-layer-diagram,
  title={BERT Embeddings},
  author={George Novak},
  howpublished={\url{https://tinkerd.net/blog/machine-learning/bert-embeddings/}},
  year={2023},
  month={March}}
  
@misc{jurafsky-speech,
  title={Speech and language processing},
  author={Jurafsky, Daniel},
  year={2000},
  publisher={Prentice-Hall}
}

@misc{mikolov-word2vec,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@misc{ffnn,
  title={The Feedforward Demystified: A Core Operation of Transformers},
  author={Kye Gomez},
  year={2023},
  month={December},
  howpublished={\url{https://medium.com/@kyeg/the-feedforward-demystified-a-core-operation-of-transformers-afcd3a136c4c}}}

@misc{fine-tune-bert,
      title={How to Fine-Tune BERT for Text Classification?}, 
      author={Chi Sun and Xipeng Qiu and Yige Xu and Xuanjing Huang},
      year={2020},
      eprint={1905.05583},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.05583}, 
}

@inbook{backpropagation,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title = {Learning representations by back-propagating errors},
year = {1988},
isbn = {0262010976},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Neurocomputing: Foundations of Research},
pages = {696–699},
numpages = {4}
}

@inproceedings{sentiment-cls-diagram,
author = {Nuraliza, Hilda and Pratiwi, Oktariani and Lubis, Muharman},
year = {2023},
month = {08},
pages = {207-212},
title = {Metaverse Tweet Sentiment Text Classification Using Bert Algorithm and Tunning Hyperparameter},
doi = {10.1109/ICE3IS59323.2023.10335255}
}

@article{sentence-pair-cls-diagram,
author = {Chang, Ting and Fan, Yao-Chung and Chen, Arbee},
year = {2022},
month = {05},
pages = {},
title = {Emotion-cause pair extraction based on machine reading comprehension model},
volume = {81},
journal = {Multimedia Tools and Applications},
doi = {10.1007/s11042-022-13110-9}
}

@article{softmax,
  title={Softmax units for multinoulli output distributions. Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  journal={Preprint at},
  year={2018}
}

@misc{colab,
  title={Google Colab},
  howpublished={\url{https://colab.google/}},
  year={2024},
  month={December}}
  
@misc{huggingface-transformers,
  title={Hugging Face Transformers},
  howpublished={\url{https://huggingface.co/docs/transformers/en/index}},
  year={2024},
  month={December}}
  
@misc{pytorch,
  title={PyTorch},
  howpublished={\url{https://pytorch.org/}},
  year={2024},
  month={December}}
  
@misc{twitter-costs,
  title={Twitter announces new API pricing, posing a challenge for small developers},
  howpublished={\url{https://www.theverge.com/2023/3/30/23662832/twitter-api-tiers-free-bot-novelty-accounts-basic-enterprice-monthly-price}},
  author={Jon Porter}
  year={2023},
  month={May}}

@misc{gigo,
  title={Garbage In, Garbage Out (GIGO)},
  howpublished={\url{https://www.techtarget.com/searchsoftwarequality/definition/garbage-in-garbage-out}},
  author={Rahul Awati}
  year={2024},
  month={December}}
  
@inproceedings{vader,
author = {Hutto, C.J. and Gilbert, Eric},
year = {2015},
month = {01},
pages = {},
title = {VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text},
journal = {Proceedings of the 8th International Conference on Weblogs and Social Media, ICWSM 2014}
}

@misc{textblob,
  title={TextBlob},
  howpublished={\url{https://textblob.readthedocs.io/en/dev/}},
  year={2024},
  month={December}}
  
 @misc{tabsa,
      title={SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods}, 
      author={Marzieh Saeidi and Guillaume Bouchard and Maria Liakata and Sebastian Riedel},
      year={2016},
      eprint={1610.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1610.03771}, 
}

@inproceedings{sun,
    title = "Utilizing {BERT} for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
    author = "Sun, Chi  and
      Huang, Luyao  and
      Qiu, Xipeng",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1035",
    doi = "10.18653/v1/N19-1035",
    pages = "380--385",
    abstract = "Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets. The source codes are available at \url{https://github.com/HSLCY/ABSA-BERT-pair}.",
}

@misc{optuna,
  title={Optuna},
  howpublished={\url{https://optuna.org/}},
  year={2024},
  month={December}}
  
@article{hyperparameters,
title = {Understanding the Effect of Hyperparameter Optimization on Machine Learning Models for Structure Design Problems},
journal = {Computer-Aided Design},
volume = {135},
pages = {103013},
year = {2021},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2021.103013},
url = {https://www.sciencedirect.com/science/article/pii/S0010448521000245},
author = {Xianping Du and Hongyi Xu and Feng Zhu},
keywords = {Structure design, Surrogate models, Machine learning, Hyperparameters optimization},
}

@misc{anand,
  title = {{FPL Historical Dataset}},
  author = {Anand, Vaastav},
  year = {2025},
  howpublished = {Retrieved February 2025 from \url{https://github.com/vaastav/Fantasy-Premier-League/}}
}

@misc{lightGBM-vs-XGBoost,
  title = {{Light GBM vs XGBOOST}},
  author = {Pranjal},
  year = {2017},
  howpublished = {Retrieved February 2025 from \url{https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/#h-lightgbm-vs-xgboost}}
}

@misc{lightGBM,
  title = {{Light GBM}},
  author = {Geeks for Geeks},
  year = {2025},
  howpublished = {Retrieved February 2025 from \url{https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine}}
}


