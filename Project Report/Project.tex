% -------------------------------------------------------------
% CONFIGURE FILE
\documentclass{report}
\setlength{\parindent}{0pt}
% -------------------------------------------------------------
% PACKAGES 
\usepackage{xcolor}
\usepackage[normalem]{ulem} 
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{parskip}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage[justification=centering]{caption}
\DeclareMathOperator*{\argmin}{argmin}
% -------------------------------------------------------------
% START DOCUMENT CONTENT
% -------------------------------------------------------------
\begin{document}   
\pagecolor{white}
% -------------------------------------------------------------
% COVER PAGE
% -------------------------------------------------------------
\begin{center}
	\includegraphics[scale=0.5]{lu-logo}

	{\vspace{3em}}

	\Huge{{\textbf{Sentiment Analysis of Twitter to Optimise Fantasy Premier League Performance}}}

	{\vspace{1em}}

	\LARGE{By Lewis Watt}

	{\vspace{2em}

	\Large{
	Loughborough University\\
	Student ID: F125967\\
	24COC251: Computer Science Project}

	{\vspace{2em}}

	\Large{
	Supervisor: Magda Zajaczkowska\\
	Submitted: }}
\end{center}
% -------------------------------------------------------------
% CONTENTS
% -------------------------------------------------------------                          
\tableofcontents 
% -------------------------------------------------------------
% INTRODUCTION
% -------------------------------------------------------------
\chapter[Introduction]{Introduction}
% -------------------------------------------------------------
% BACKGROUND
% -------------------------------------------------------------
\section[Background]{Background}
Fantasy Premier League (FPL) is a popular online sports game based around the real-life top tier of English Football, the Premier League. Players of the game, often referred to as 'managers', are tasked with selecting a squad of 15 real-life premier league players every week. In game players are assigned a monetary value ranging from £4-15m, and each manager has a budget of £100m to build their team. The real-life performance of players determines how many points they are rewarded each week. Positive actions like scoring goals and providing assists increase points, whilst negative actions like receiving a red card or scoring an own-goal result in a decrease in points. Over the course of a 38-gameweek season, managers aim to score the most amount of points by rotating players in and out of their teams as they see fit.

The first version of FPL was launched in 2002 alongside the launch of the Premier League website, ahead of the 2002/3 season. Around 75,000 players participated in the first season \cite{fpl-origins}, and since then growth has been explosive, with the player count surpassing 10 million in the 2022/23 season \cite{fpl-playercount}. Each season the game has become more competitive, with the Premier League offering incentives such as a 7-night break inclusive of VIP hospitality tickets at two 2025/26 Premier League matches for the winner of the current season \cite{fpl-prizes}. As well as the official prizes, many people participate in 'money leagues' where a small fee is paid for entry, and the winner takes home the prize pot. Alongside this more casual betting, professional gambling companies have started offering fantasy themed games where participants often stake money on day or weekend long periods, choosing a fantasy team to try and win them money by scoring the most points. With the global fantasy sports market valued at over \$27 billion in 2022, and projected to reach \$87 billion in 2031\cite{fpl-market-growth}, it is no surprise companies are scrambling to try and capitalise off of the immense popularity of the game.

With the increased popularity of FPL and a rapidly growing market full of potential customers, AI-based platforms aimed at increasing customers' overall ranks for little effort have started to emerge. These tools offer managers a quick and easy way to put together a team that they know will perform at a decent level, taking the effort out of the game for the managers who don't want to spend time diving through data and stats. AI tools also take the bias out of the game, as people tend to just pick their favourite players or players from their favourite teams, regardless of how well they are likely to perform in the game. Whether it be for monetary reasons, or for the simple pleasure of getting bragging rights over friends, the use of these platforms has shot up particularly in the last 2 years. One of the biggest AI platforms is \textit{Fantasy Football Hub} \cite{ffh}, which launched in 2019 and has grown to over 40,000 paying users, with around £2.5m in annual revenue \cite{ffh-users}. The platform uses a longitudinal multilevel regression model \cite{multilevel-models} in combination with the football data platform \textit{Opta} to analyse each player's potential. The model is used to offer users of the platform recommendations for transfers, alongside a spreadsheet of points players are expected to score each week. By using this information to deliver customers clear and precise recommendations, \textit{Fantasy Football Hub} has been successful in capturing a large customer base who can market their platform through word of mouth.

Another tool people consult when making FPL related decisions is social media, where users often share their teams with each other and discuss potential players to buy or sell. It was found that over 70\% of FPL players find social media to be at least somewhat influential when it comes to making decisions around their team \cite{whittaker}. The FPL subreddit r/FantasyPL has over 700k members \cite{r/FPL}, and the official FPL Twitter account has a whopping 6.2m followers \cite{@OfficialFPL}. As well as the official accounts run by the Premier League themselves, many so called 'experts' have started to gain a large following on Twitter. Accounts such as \texttt{@FPLGeneral}, \texttt{@FFScout}, \texttt{@BenCrellin}, and \texttt{@LetsTalk\_FPL} all have well over 250k followers, where useful tips and insights are often posted to help followers decide who to buy and sell each week.

However, using social media can often be a tedious process, sifting through thousands of posts trying to decide which ones to listen to - a stark contrast to the ease of use AI platforms bring. By leveraging sentiment analysis, this manual process can be automated, enabling AI to sift through vast amounts of data and extract meaningful insights. This study aims to explore how sentiment analysis can be applied to social media data and used in combination with existing AI models to enhance the accuracy of recommendations for FPL managers. In doing so it adds to the ongoing discussion around integrating human feedback into existing AI models used to forecast FPL performance. This study hopes to explore new areas with a particular emphasis on Twitter, a platform previously neglected when it comes to FPL research, due to the complicated jargon found in typical posts.
% -------------------------------------------------------------
% FPL RULES
% -------------------------------------------------------------
\section[Rules of FPL]{Rules of FPL}
\label{sec:rules-of-fpl}
Note the terms \textit{player} and \textit{manager} are often confused when it comes to FPL, so please be aware that this study uses the term \textit{manager} to refer to people who participate in the game of FPL, and \textit{players} to refer to real-life premier league footballers.

Fantasy Premier League is a mirror of the real-life English Premier League, meaning that each in game event (referred to as a \textit{gameweek}) represents a set of real-life football matches. The game takes place over a \textit{season} which usually runs from August until May of the following calendar year, and at the end of the season the winner is crowned. There are 20 teams who play each other twice over the course of the season, resulting in a total of 38 gameweeks. Note that sometimes due to unforeseen circumstances like bad weather or domestic cup fixtures, games are postponed resulting in a team not playing in one gameweek (known as a \textit{blank} gameweek), and playing twice in another gameweek (known as a \textit{double} gameweek) to make up.

Upon the start of the season, each manager is tasked with choosing a team of players from a database of around 500. A team must be made up of 15 players, consisting of 2 goalkeepers (GKs), 5 defenders (DEFs), 5 midfielders (MIDs), and 3 strikers (STs). On top of these positional constraints, each player is assigned a monetary value in £m's (roughly according to how well they performed in the previous season), and the manager is given a budget of £100m from which they must select their team. A final constraint on team selection is that you cannot select more than 3 players from the same team, so it would not be possible to buy the whole Manchester United team, for example.

	% -------------------------------------------------------------
	%INITIAL TEAM SELECTION FIGURE
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.275]{team-selection-initial}
	\caption{Initial Team Selection}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	%INITIAL TEAM SELECTION FIGURE
	% -------------------------------------------------------------
	
Replacing a player in your team with another is known as a \textit{transfer}. Each manager is given 1 free transfer (\textit{FT}) every gameweek, which can accumulate up to a maximum of 5. Managers who make a number of transfers that is higher than their free transfer balance must pay 4 points per additional transfer - referred to as a \textit{hit}. Transfers cannot lead to illegal teams, i.e. teams resulting from transfers must still meet the positional, budgetary, and 3-player per team constraints mentioned above.

Another variable affecting team selection is price changes. Throughout the season player's values will go up and down depending on how popular they are among the player base. The more people buying a player, the more expensive they become and vice versa. This means managers can grow their budget by selling players who have gone up in value. When a manager sells a player who has gone up in value, the manager receives 50\% of that increase rounded down to the nearest £0.1m. So if a manager bought a player for £5m and sells them at £5.5m, the manager will only receive £5.2m back. However, when a player goes down in value, the manager suffers the full price loss.

Every gameweek, managers must choose 11 \textit{starters} (a \textit{starting 11}) and 4 \textit{substitutes} or \textit{'subs'}. The starting 11 is the set of players who contribute to a manager's overall score (points scored by substitutes do not count towards their total). A starting 11 must consist of at least 3 DEFs, 3 MIDs, 1 ST, and exactly 1 GK. Subs will replace starters in the event that the starting player does not play a single minute in their real-life match. Before the start of the gameweek, the manager must choose a substitution preference, which is the order in which substitutes replace starters. This preference does not supersede the starting 11 constraints - that is, if a defender did not play in real-life and a midfielder was first in the substitute preference, the midfielder would not replace the defender if it resulted in a team with less than 3 defenders.

One of the most important choices a manger must make is who to \textit{captain}. Giving a player captaincy places a 2x multiplier on their points, meaning if they were to score 10 points the manager would receive 20. A vice-captain is also chosen, and in the case that the captain is replaced by a substitute they will become the new captain.

A gameweek \textit{deadline} is the final opportunity for managers to make changes to their teams for that gameweek. This deadline always occurs 90 minutes before the first real-life match of the gameweek. Once the deadline passes, any changes made will apply only to the following gameweek. Points are then awarded based on the outcomes of the matches over the next few days. This process of team selection, deadlines, and scoring continues throughout the season, resulting in a leaderboard ranking that determines the winner. The points scoring system can be found \href{https://www.premierleague.com/news/2174909}{\textcolor{blue}{\uline{here}}}
% -------------------------------------------------------------
% PROJECT AIMS AND OBJECTVES
% -------------------------------------------------------------
\section[Project Aims and Objectives]{Project Aims and Objectives}
\subsection[Goals]{Goals}
The main goal of the project is to create a new model that will incorporate Twitter data about English Premier League players as an input. Using a fine-tuned sentiment analysis model to analyse the Twitter data, the sentiment towards each player should be incorporated into the new model to create a forecast of a player's expected points (xP) for each \textit{gameweek}. A team selection algorithm to pick an optimal squad based on each player's xP and price will then be created to evaluate the overall performance of the model and compare it to existing models. These include models that use other sources of data for sentiment such as news articles and betting markets, as well as model with no sentiment information.

To the knowledge of the author, no research has been done surrounding sentiment analysis of Twitter for player performance forecasting in the FPL domain. This study aims to advance research in this area and further contribute to the existing fantasy sports forecasting research. The results of the model will be tested against existing research and the most popular AI FPL platforms. As well as this, models that have used sentiment analysis of other forms of media such as news articles will be compared to test the suitability of using Twitter as a data source. The model will be evaluated over a range of gameweeks and the team picked from the team selection algorithm will have its overall score compared against other prediction methods to see where it ranks in the global leaderboard.
\subsection[Objectives]{Objectives}
\begin{enumerate}
    \item Analyse the project aims and break them down into key functional and non-functional requirements.
    \item Conduct a literature review on fantasy sports forecasting and sentiment analysis techniques, as well as research on existing FPL forecasting platforms.
    \item Fine-tune a sentiment analysis model specifically for capturing player sentiment in the FPL context, using labeled data for model training and validation.
    \item Develop and validate a predictive model that uses Twitter sentiment, historical player data, and match fixtures to forecast a player's expected points (xP) for each gameweek.
    \item Implement a team selection algorithm that optimises squad selection based on player xP, prices, and FPL constraints, ensuring compatibility with official game rules.
    \item Test and evaluate the model by using the chosen team to simulate its performance over multiple gameweeks against existing FPL forecasting methods, both with and without sentiment analysis.
    \item Prepare a comprehensive report documenting the findings, challenges, and recommendations for future work by the end of the project.
\end{enumerate}

\subsection[Statistical Objectives]{Statistical Objectives}
\begin{itemize}
    \item The main goal of the model is to achieve a higher overall rank than existing models over the course of an FPL season.
    \item Evaluate the effectiveness of Twitter sentiment analysis in improving xP prediction accuracy by comparing the root mean squared error (RMSE) of models with and without sentiment analysis.
    \item Assess the impact of the sentiment based model on team performance by comparing the total points scored by squads selected with and without sentiment-informed xP predictions.
    \item Measure the success of the team selection algorithm by analysing its ability to pick the best players from all combinations.
    \item Benchmark the model's results against popular AI-powered FPL platforms by comparing overall ranks, total points, and average weekly points.
    \item Compare data sources by conducting a statistical comparison against models using sentiment derived from other sources such as news articles.
    \item Ensure sufficient long-term prediction ability by analysing model performance across multiple gameweeks, accounting for variability in player form, injuries, and match conditions.
\end{itemize}
% -------------------------------------------------------------
% LITERATURE REVIEW
% -------------------------------------------------------------
\chapter[Literature Review]{Literature Review}
Due to the huge popularity of not only Fantasy Premier League, but the multiple games available for different sports under the fantasy sports genre, there exists a wide plethora of research and discussion around the given problem of maximising performance in fantasy sports. This chapter looks at the existing models that have been created and reviews how different machine learning methods stack up against each other. The existing research suggests that whilst many different techniques have been applied to varying degrees of success, using Twitter sentiment to predict player performance over the course of a whole season is a novel approach to the fantasy sports optimisation problem. The theory of wisdom of crowds and human-feedback aided models are explored to verify the benefits of using Twitter as a source of information. Finally, existing sentiment analysis methods are analysed to see how natural language is processed and evaluated to gather an overall sentiment value. The combination of all the research gives a good understanding for the necessity and validity of the approach this study focuses on. 
% -------------------------------------------------------------
% FANTASY SPORTS FORECASTING
% -------------------------------------------------------------
\section[Fantasy Sports Forecasting]{Fantasy Sports Forecasting}
The problem of optimising a player's score over the course of a whole fantasy season is an extremely difficult task. The magnitude of this difficulty was illustrated by Kristiansen et al. who developed a mathematical model to describe fantasy premier league, and found that during the 2017/18 season the top manager in the world only managed to achieve a score equivalent to 51.75\% of the optimal solution \cite{kristiansen}. Furthermore, the mean gap between the optimal solution and the top manager was around 60 points per week, whilst the mean gap between the top manager and the average manager was only 20 points per week. This huge gap highlights the complex intricacies of fantasy sports optimisation, where the sheer number of variables like player mood, injury status, and team rotation make it almost impossible for human strategies to get anywhere close to an optimal solution. The large discrepancy between optimality and the current peak of human performance shows the potential for advanced machine learning methods to bring new insights and strategies that could help bridge the gap between human strategies and optimal performance.

In terms of existing mathematical and machine learning models, many solutions for predicting performance have already been created by researchers dating back to the early 2000s. These models all largely focus on using historical data with objective, performance-based metrics that describe exactly what a player or team has done in the past few weeks, combined with some sort of score that represents how difficult their upcoming games are going to be. In 2012 Matthews et al. modelled the problem of choosing a team as a belief-state Markov decision process, where player selections were actions that had some reward value \cite{matthews}. Using a Bayesian Q-learning algorithm actions were chosen to maximise long-term reward, resulting in a team expected to score the most points. This approach retrospectively would have ranked within the top 1\% of all managers during the 2010/11 season of FPL.

In 2018 GS created a binary classification model to classify players into groups that are 1 - expected to score at least 4 points in the next gameweek, and 2 - expected to score less than 4 points \cite{raghunandh}. He experimented with using different tree models but eventually concluded using a Gradient Boost model was the most accurate, using historical performance data to generate a number between 0 and 1 for each player. A threshold value was then determined to best split the players into the two separate categories. This approach proved successful, with the model achieving a precision of 83\% when choosing players expected to score at least 4 points in the next gameweek.

In 2022 Rajesh et al. used random forests and gradient boosting machines to create a system enabling the "average interested person" to make better decisions about who to include in their teams \cite{rajesh}. A key feature found in this study was that training multiple models for each playing position (goalkeeper, striker etc.) drastically increased performance in comparison with using one model for each position. All models used in this study were found to outperform the average player by at least 20\%, with Gradient Boosting Machines (GBMs) performing the best.

Also In 2022 Bangdiwala et al. compared three different methods - Linear Regression, Decision Trees, and Random Forests - for predicting the total number of points players would score in their upcoming match \cite{bangdiwala}. He found that over the course of a whole season, the Linear Regression model performed the best with a smaller root mean square error and mean absolute error than the other two models. The Random Forest model was a close second and the Decision Tree model performed the worst. Another study in 2024 by Papageorgiou et al. compared 14 models for fantasy basketball and found the most effective models to be Random Forests, Bayesian Ridge and AdaBoost \cite{papageorgiou}. 
% -------------------------------------------------------------
% WISDOM OF CROWDS
% -------------------------------------------------------------
\section[Wisdom of Crowds]{Wisdom of Crowds}
In his 2005 book \textit{The Wisdom of Crowds}, James Surowiecki explains how collective groups are often much better at predicting things than individuals \cite{wisdom-of-crowds}. Aristotle is credited as the first person to write about this theory in his work \textit{Politics}. He stated "it is possible that the many, though not individually good men, yet when they come together may be better, not individually but collectively, than those who are so, just as public dinners to which many contribute are better than those supplied at one man's cost" \cite{politics}. This is illustrated in Francis Galton's \textit{Vox Populi} where he describes a country fair in Plymouth in 1906 \cite{galton}. During the fair, a contest was being held to guess the weight of an ox. Galton observed that the median guess of a crowd of 800 people was within 1\% accuracy of the correct number. Suroweicki uses this example to explain how "a crowd's individual judgement can be modelled as a probability distribution of responses, with the median value being close to the true value of the predicted quantity"\cite{wisdom-of-crowds}.

An expansion on this theory is a new technique dubbed "surprisingly popular" \cite{surprisingly-popular}. This technique was discovered during a study at MIT's Sloan Neuroeconomics Lab in collaboration with Princeton University. In the study, participants were asked a series of questions for which they had to provide what they thought was the correct answer, alongside what they thought the most popular answer would be. Researchers found that taking the average difference between the two responses as the correct answer, reduced errors by 21.3 in comparison to simple majority results, and 24.2 percent in comparison to confidence weighted results, where participants gave a confidence score alongside their answer to express how confident they were with their answer.

Taking this knowledge back into an FPL context, research exists detailing how the use of crowd-based metrics such as a player's ownership percentage among other mangers can benefit overall performance. In the earlier mentioned model built by GS, he states how using the concept of wisdom of crowds by adding features like player ownership\%, transfers in, and transfers out to his gradient boost model improved the precision from 75\% to 83.33\% \cite{raghunandh}. Another study in 2019 by Bonello et al. details how expanding on existing models with human feedback such as news articles and betting markets led to a performance increase of over 300 points over the course of a whole season, ranking within the top 0.5\% of players in the world \cite{bonello}. This is compared to a standard statistical model that only placed within the top 13\%. The Bonello study also details how they explicitly decided against the use of Twitter posts in their model, as they found it hard to accurately derive sentiment from tweets due to grammatical errors, emoji usage, and football specific jargon. However, this study aims to use proper pre-processing and more accurate sentiment analysis methods that have emerged since 2019 (detailed in subsequent chapters) to eliminate these concerns, as there is a huge amount of data available on Twitter from a diverse crowd of people that should not be overlooked.

This is backed up by a 2022 study where Whittaker found that 72\% of FPL players found social media to be at least somewhat influential in their decision making when it comes to their FPL team \cite{whittaker}. Twitter was also central to a 2019 study by Bhatt et al. where crowd wisdom was put to the test using Twitter sentiment as a metric for creating diverse crowds \cite{bhatt}. FPL related tweets were collected, and then matched to real people's FPL accounts. User's historical player selection was then collected, focusing specifically on who they chose to captain each week. The Twitter users were then clustered into diverse crowds based on the semantic diversity of their tweets, and further sampled from the clusters to create a final set of diverse crowds. Analysing the captaincy choice from these groups found that on average, the captaincy choice from a random group of 6 outperformed 73\% of individuals, and the choice of a diverse group of 6 outperformed 93\% of people.
% -------------------------------------------------------------
% SENTIMENT ANALYSIS
% -------------------------------------------------------------
\section[Sentiment Analysis]{Sentiment Analysis}
The desire to capture and give meaning to public opinions has long been seen throughout history, with democratic voting as a measure of public opinion first appearing in early Greek civilisation in the 5th century BCE \cite{athenian-democracy}. A paper by Droba published in the early 20th century outlines methods for collecting public opinion, explaining how early questionnaires were first deployed \cite{droba}. With the emergence of the internet in the last few decades, methods of gathering public opinion have shifted online making it much more efficient for organisations to gather relevant information. Companies have become interested in gathering opinions about their products or services through online reviews. With the explosion in popularity of social media, individual researchers have been able to gather information for tasks like predicting elections, stock market trends, and natural disasters \cite{mantyla}.

Liu defines sentiment analysis or opinion mining as the field of study that analyses people's opinions, sentiments, evaluations, attitudes, and emotions from written language \cite{liu}. Patel et al. adds to this definition explaining it is a type of classification in which machine learning techniques are used to identify positive and negative words or reviews in text-driven databases \cite{patel}. Shah outlines the different methods that can be used for sentiment analysis, explaining their pros and cons \cite{shah}. In her article she explains the different machine learning models that are commonly used like the Naive Bayes algorithm that uses probability to determine whether a piece of text should be classified as positive, negative, or neutral. Recurrent Neural Networks (RNNs) and their variants Long Short-term Memory (LSTM) are mentioned due to their ability to handle long term dependencies often found in natural language.

In 2019, a new language representation model called Bidirectional Encoder Representation from Transformers (BERT) was introduced by Devlin et al. with a focus on pre-training deep bidirectional representations by jointly conditioning on both left and right context in all layers \cite{BERT}. The ability of BERT to understand language context from both directions (unlike LSTM) more accurately meant it set new benchmarks in the natural language processing community, and has become a cornerstone of modern NLP research. BERT can be fine tuned on a multitude of tasks including sentiment analysis, and a study by Elankath et al. found that when used for sentiment analysis of Malayalam tweets, BERT was the top performing model in terms of accuracy when compared to other models like LSTM \cite{elankath}.

Targeted Aspect-Based Sentiment Analysis (TABSA) aims to determine sentiment toward specific targets, such as individuals or entities \cite{tabsa}. This idea is particularly important in the context of FPL sentiment analysis because tweets often mention multiple players. Identifying sentiment toward specific players is far more valuable than assessing the overall sentiment of a tweet. Sun et al. proposed a methodology using BERT to address TABSA by framing it as a sentence-pair classification task \cite{sun}. Their approach involves constructing an auxiliary sentence, such as "What do you think of the safety of location - 1?", and pairing it with the relevant context to identify the sentiment toward the specific target. Building on this idea, Hoang et al. advanced the methodology in 2019, achieving state-of-the-art results across various sentiment analysis benchmarks \cite{hoang}.
% -------------------------------------------------------------
% SUMMARY
% -------------------------------------------------------------
\section[Summary]{Summary}
From the research explored, it is clear that there is a large gap between the points scored by the top performing manager in the world and the ceiling of potential points available. This gap highlights the potentially undiscovered strategies that machine learning models could discover to help bridge that gap and outperform other human managers. The existing models used for optimising FPL performance have been able to perform well with some ranking inside the top 1\% of all players in the world. Of these models, some of the best performing include Random Forests, Linear Regression and Gradient Boosting Machines. It is also clear that incorporating human feedback and crowd wisdom into these models helps to improve their performance by a significant margin. Using high-performing deep learning models like BERT can give accurate sentiment analysis of tweets, whilst targeted aspect-based sentiment analysis can further improve this by capturing sentiment towards people and groups.
% -------------------------------------------------------------
% THEORY
% -------------------------------------------------------------
\chapter[Theory]{Theory}
The purpose of this chapter is to provide a deeper understanding of the fundamental machine learning methods and models used in this project. Additionally, it highlights the key theoretical concepts learned during the project's development. Whilst a model could technically have been developed without any of this knowledge, its addition has been extremely helpful in shaping the direction of development.

The implementation makes use of various APIs, which often function as 'black boxes' offering predefined methods for training and evaluation without revealing their internal workings. Gaining insight into the underlying processes of these API calls has been essential in refining the model. It has not only improved interpretability but also allowed for more effective debugging, as errors and unexpected behaviours become clearer when understood in context.

A strong theoretical foundation has also helped when it comes to model selection, feature engineering and hyper-parameter optimisation. For example, understanding the key differences in gradient boosting methods with their pros and cons has helped to ensure consideration for bias-variance trade-offs by avoiding overfitting. By including detailed explanations of these concepts it is hoped the reader gains a greater appreciation for the choices taken in this project.

\section[The BERT Model]{The BERT Model}
Bidirectional Encoder Representation from Transformers or BERT \cite{BERT}, is a pre-trained language representation model based on the transformer architecture proposed in the now infamous 2017 paper "Attention is All You Need" \cite{attention}. The transformer is a deep learning architecture that is the backbone of some of the most prominent achievements in AI in recent years, including OpenAI's ChatGPT \cite{chatGPT}.

        % -------------------------------------------------------------
	% TRANSFORMER ARCHITECTURE
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.37]{transformer-architecture}
	\caption{Architecture of a Transformer \cite{attention}}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% TRANSFORMER ARCHITECTURE
	% -------------------------------------------------------------

The original transformer architecture was designed for language translation, but has since been adopted and used for a vast array of other tasks, including for pre-trained systems like BERT and Generative Pre-trained Transformers (GPTs) \cite{transformer-applications}. The transformer is made up of two parts; the encoder, and the decoder. A few months after the transformer architecture was proposed, researchers started experimenting with the idea of separating the encoder and the decoder resulting in some incredible breakthroughs. The creation of encoder-only and decoder-only transformers is what has resulted in the AI boom that we have seen today with large language models like GPTs \cite{chatGPT} and BERT \cite{BERT}. BERT is made up of deeply bidirectional encoder-only transformers which although have been more understated than their decoder-only siblings (used in ChatGPT), are still extremely powerful for natural language tasks like sentiment analysis.

\subsection*{Tokenisation}
To split large pieces of text into more manageable chunks of data so that natural language models can process and understand them better, a process known as tokenisation is carried out on the training data \cite{jurafsky-speech}.

In the BERT model, raw text is broken down into tokens using the WordPiece tokenisation algorithm \cite{word-piece}. As opposed to word-level or character-level tokenisation, where whole words or individual characters represent tokens, the WordPiece algorithm is a subword-level tokenisation algorithm. This means words are split into one or more tokens such as \texttt{'token'} and \texttt{'isation'}.

The algorithm starts by generating an initial vocabulary from the training data, by splitting each word in the data into individual characters. Each character that does not start a word is prefixed with \texttt{'\#\#'} to indicate it is a sub-word, so the word 'word' would be split like this: \texttt{'w' '\#\#o' '\#\#r' '\#\#d'}. WordPiece then computes a score for each pair that occurs in the training data using the following formula:

\[
\text{score} = \frac{\texttt{pair\_freq}}{\texttt{first\_element\_freq} \times \texttt{second\_element\_freq}}
\]

The pair with the highest score is merged into 1 token and added to the vocabulary, and the same merge is applied to the set of pairs of tokens. This process is then repeated until the vocabulary reaches a desired size. The BERT vocabulary is sized around 30k tokens \cite{BERT}.

Once the vocabulary has been generated, words can be tokenised. This is done by iterating through each word in a piece of text, and looking for the longest available token at the start of a word. If one is found, the algorithm does the same for the next part of the word, and so on until the whole word is tokenised. If the algorithm finds a part of a word that has no matching token in the vocabulary, the \textbf{entire word} is given the special token [UNK] or unknown \cite{word-piece}. So if you had a vocabulary of ['hel', 'lo', 'worl'] and the sentence 'hello world', the resulting tokenisation output would be ['hel', 'lo', [UNK]].

The BERT model also uses the special tokens [CLS] (classifier) and [SEP] (separator). The CLS token is placed at the very start of the input, and for sentiment analysis tasks it has a hidden state associated with it that will be passed to a classifier to predict sentiment after the input has been processed \cite{fine-tune-bert}. The SEP token is used to separate two sentences for tasks such as question-answering and dual-sentence classification \cite{BERT}.

\subsection*{Input Embeddings}
        % -------------------------------------------------------------
	% EMBEDDING LAYER DIAGRAM
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.5]{bert-embedding-layer}
	\caption{BERT Embedding Layer \cite{embedding-layer-diagram}}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% EMBEDDING LAYER DIAGRAM
	% -------------------------------------------------------------
After text has been tokenised, it needs to be converted into numerical values using text encoding. Text encoding allows raw text to be handled by neural networks which can only take numerical values as input \cite{jurafsky-speech}. The raw text is converted into a set of vectors called word embeddings, which can be processed by the encoder's neural network. Embeddings give meaning to tokens, with similar tokens like 'great' and 'awesome' closer together in vector space, and opposites like 'sad' and 'happy' further apart \cite{mikolov-word2vec}.

The embedding layer creates word embeddings (or subword embeddings, in the case of BERT) for each token in the input using 3 different types of embeddings: token embeddings, positional embeddings, and token type embeddings \cite{embedding-layer-diagram}. 

\begin{itemize}
    \item \textbf{Token embeddings} are vectors that have been learned during the model's pre-training phase to place similar tokens close together in vector space, they are stored in an embedding matrix which maps each token to a specific embedding. 
    \item \textbf{Positional embeddings} are also learned vectors given to each token that represent their positions in the input sentence.
    \item \textbf{Token type embeddings} are often used for two-sentence NLP tasks like question-answering to identify which sentence a token belongs to. For one-sentence tasks like next word prediction, all tokens are assigned the same vector.
\end{itemize}

The embedding layer then sums these embeddings together, and applies normalisation to their sum. The resulting vector output contains meaningful information about each token and its position in the input. These embeddings are passed into the subsequent transformer layers of the BERT model for processing.

\subsection*{Multi-Head Attention}
A fundamental concept of the transformer architecture is 'self-attention'\cite{attention}. Given the sentence: \textit{'I took the pizza out of the oven and then ate it.'} it is apparent to any human that 'it' refers to the pizza and not oven, based on the surrounding context. Neural networks however lack the ability to identify this relationship, and require mechanisms like self-attention in order to resolve such ambiguities.

Self-attention allows neural networks to identify the importance of each token in relation to the other tokens in the input. To calculate attention values, BERT uses multiple attention 'heads' which all focus on different linguistic features of the input. Each head involves 3 components: the query (Q), the key (K), and the value (V) matrices \cite{attention}. Each of these matrices are made up of the input embeddings of the previous layer (a vector for each token), and are identical to ensure that every token in the input interacts with every other token, including itself (the idea of self-attention).
	% -------------------------------------------------------------
	% DOT PRODUCT ATTENTION
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.4]{dot-product-attention}
	\caption{Scaled Dot Product Attention \cite{attention}}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% DOT PRODUCT ATTENTION
	% -------------------------------------------------------------
\[
\text{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

Here $QK^T$ computes the similarity between the key and query matrices, scaled by $\sqrt{d_k}$ (the dimension of the embedding vectors), and then the softmax function \cite{softmax} normalises these values into attention weights. These weights are applied to the value matrix, resulting in an attention net matrix representing the contextual relationship between tokens.

For all attention layer heads (BERT uses $h$=12 attention heads \cite{BERT}), there is a linear layer that uses learned weights to project the input embeddings into separate Q, K, and V matrices for each head. This way different linguistic features can be explored by the model to improve its learning ability. The outputs (attention nets) from each head are then concatenated to form a unified representation:
	% -------------------------------------------------------------
	% MULTI-HEAD ATTENTION
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.4]{multi-head-attention}
	\caption{Multi-Head Attention \cite{attention}}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% MULTI-HEAD ATTENTION
	% -------------------------------------------------------------
\[
\begin{split}
\text{MultiHead(Q, K, V)} = \mathrm{Concat}\left(\text{head}_1, \ldots, \text{head}_h\right) W^O \\
\text{where } \text{head}_i = \mathrm{Attention}\left(QW_i^Q, KW_i^K, VW_i^V\right).
\end{split}
\]

Here, $W_{i}^Q, W_{i}^K , W_{i}^V$ are the learned projection matrices for the $i$-th head, and $W^O$ is the projection matrix for the final linear layer. This final layer aggregates the information from all attention heads to produce a final self-attention representation for the input \cite{attention}.

\newpage
\subsection*{Feed-Forward Network}
The feed-forward neural network (FFNN) is the final stage of the encoder. It is a type of multi-layer perceptron (MLP), used for transforming the output of the self-attention mechanism into a refined representation that can capture deeper relationships within the input sequence \cite{ffnn}.

The FFNN processes input embeddings by first projecting them into a higher-dimensional space. Specifically, the 768-dimensional input for each BERT token embedding \cite{BERT} is transformed into a 3072-dimensional space via a linear projection. This expansion allows the model to explore more complex interactions among the input features. Following this, a ReLU activation function is applied to allow the model to identify more intricate non-linear patterns in the language. Finally, another linear projection reduces the representation back to its original dimensionality, ensuring compatibility with the encoder's output structure \cite{ffnn}.

The FFNN is essential for capturing patterns and interactions that the self-attention mechanism alone cannot model. Its reliance on simple matrix operations makes it computationally efficient and highly parallelisable, leveraging modern hardware accelerators like GPUs and multi-core CPUs to significantly reduce training time. To ensure stable training and enhance convergence, the FFNN's output undergoes normalisation, producing the final output representation for the encoder.\\

\newpage
\subsection*{Final BERT Architecture}
The previous sub-sections have explored the components of the encoder, but the BERT model actually makes use of multiple encoders stacked together (12 for BERT$_{\text{base}}$ which this study uses) \cite{BERT}. This is what makes BERT a deep-learning model, as it incorporates many layers into its structure. The stacking of multiple encoders allows the model to progressively learn more high-level representations of the input data as it passes through the layers.

Each encoder layer outputs a 768-dimensional matrix capturing increasingly complex patterns and relationships in the data at every subsequent encoder layer. The output of the final encoder layer represents the most refined and meaningful representations for each token \cite{BERT}, which can then be used for downstream tasks. For sentence classification tasks like sentiment analysis, the special token ([CLS]) is used, and its final vector representation serves as the input to the classification head. The classification head takes the input vector and outputs a score representing the model's sentiment prediction \cite{fine-tune-bert}.

The depth of the architecture, combined with the self-attention mechanism, enables BERT to model long-range dependencies effectively. This is a huge benefit for processing natural language, where relationships between words are often spread across entire sentences or paragraphs.

\subsection*{Pre-Training}
BERT was pre-trained using two tasks: Next Sentence Prediction (NSP) and Masked Language Modelling (MLM) \cite{BERT}.

The first task, MLM, is a technique for training deeply bidirectional models like BERT. Typical language models process tokens sequentially left-to-right, using preceding tokens in a sentence to predict the next. BERT, however, processes all tokens in a sentence simultaneously and can see both preceding and following tokens for each word. To predict a missing token in the middle of a sentence, BERT could "cheat" by leveraging full context, including the token to predict. To address this, word "masks" are applied to randomly hide tokens in a sentence \cite{BERT}, tasking BERT to predict them using only surrounding context. This technique enables BERT to learn bidirectional relationships, resulting in a more robust model than typical left-to-right ones.

The second task, NSP, teaches BERT to understand relationships between two sentences, useful for downstream tasks like Question Answering (QA) and Natural Language Inference (NLI). In this task, BERT is fed two input sentences, A and B, and must guess whether sentence B follows A or is unrelated. BERT is trained on an even split of positive and negative cases. This simple task produces remarkable results, with the final model achieving 97\% accuracy on NSP tasks \cite{BERT}.

By leveraging these tasks across millions of sentences and performing millions of optimisation steps, BERT learns complex relationships between words and sentences \cite{BERT}. The model's weights, including those for input embeddings and self-attention mechanisms, are updated via backpropagation \cite{backpropagation} during this process.

	% -------------------------------------------------------------
	% BERT PRE-TRAINING
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.3]{bert-pre-training}
	\caption{BERT Pre-Training \cite{BERT}}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% BERT PRE-TRAINING
	% -------------------------------------------------------------
	
This extensive pre-training enables BERT to capture nuanced patterns and relationships in text, which are then fine-tuned on downstream tasks like sentiment analysis, question answering, and named entity recognition \cite{fine-tune-bert}.
% -------------------------------------------------------------
% GRADIENT BOOSTING AND DECISION TREES
% -------------------------------------------------------------
\section[Gradient Boosting and Decision Trees]{Gradient Boosting and Decision Trees}
Gradient Boosting is a machine learning technique used for regression and classification tasks that focuses on creating a large group of small, relatively poor models and then combining them into a single powerful and efficient model. These smaller models are typically what is known as a decision tree, a simple structure that makes predictions by splitting data based on feature values. 

In order to improve the model iteratively, each new decision tree is trained to correct the residuals (actual - predicted values) of the last one. All models' answers are weighted and then summed, with the result taken as the final prediction. The process of creating new trees is repeated until a set limit is reached, or additional trees fail to improve the fit.

Compared to other ensemble methods (ones which combine multiple models) like bagging and random forests which train multiple models independently on different parts of the data, gradient boosting differs as it trains new models sequentially, based on the errors of the previous one. Each tree learns the mistakes of its predecessor, making boosting a more adaptive technique.

\newpage
\subsection*{Decision Trees}
Decision trees \cite{decision-trees} are simple structures used for both classification and regression, that contain 3 important parts: the root node, decision nodes, and leaf nodes. These components make a up a flowchart-like tree where given some data, the tree can be traversed according to attribute values e.g. "age $>$ 50" until a terminal or leaf node is reached that contains the prediction e.g. "is bald" or "is not bald".

	% -------------------------------------------------------------
	% DECISION TREE EXAMPLE
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.38]{decision_tree}
	\caption{A simple decision tree to predict baldness.}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% DECISION TREE EXAMPLE
	% -------------------------------------------------------------

Decision trees can be built using various different algorithms, although they all share the goal of trying to find the attribute that best splits the data into groups. This means trees are built in such a way as to ensure they are as small as possible whilst still providing an accurate prediction. A few metrics used to measure how well an attribute splits data include: Gini impurity, entropy and mean squared error (MSE) \cite{decision-trees}. 

Whilst decision trees are simple and easy to understand, they are vulnerable to overfitting as complex trees can result in a scenario where trees start to memorise the training data rather than picking decisions that result in good general performance. To combat this techniques such as pruning can be used, and ensemble methods that build multiple trees such as gradient boosting are preferred for more complex datasets. 

\subsection*{The Gradient Boosting Algorithm}
The gradient boost algorithm builds on decision trees by using multiple decision trees that are sequentially improved to create a final model that avoids overly complex trees prone to overfitting data. 

Initially an input dataset consisting of $n$ rows is taken. Each row contains some predictor values ($x_i$) and the target value ($y_i$). 

To measure how well the model's predictions stack up against the true values, a \textbf{loss function} is used. This compares the model's predictions $F(x)$ with the observed values $y$. For regression tasks, mean squared error is typically used:
\[\frac{1}{2} (Observed - Predicted)^2\]

The $\frac{1}{2}$ is used at the front, so that calculations are simplified when derivatives are taken (the chain rule means the squared power cancels out with $\frac{1}{2}$).

\paragraph{Step 1: Initial Prediction} \leavevmode

The initial model prediction is simply the average of all known values. This is chosen as it minimises the total error across the dataset, and is determined using the formula:
\[F_0(x) = \argmin_\gamma{\sum^{n}_{i=1}L(y_i, \gamma)}\]
which simply states the predicted value $F_0(x)$ should be the one that minimises the loss function $L(y_i, \gamma)$. To calculate this the derivative of the loss function is taken with respect to $F_0$ and set to 0, resulting in a formula for the average:
\[F_0(x) = \frac{\sum^{n}_{i=1}y_i}{n}\]

\paragraph{Step 2: Building Decision Trees} \leavevmode

After the initial prediction, a set number $M$ (typically 100) of decision trees is created. To create a new tree $m$, first the residuals of the old tree are calculated:
 \[r_{im} = (Observed - Predicted) \text{ for }\: i = 1,...,n \] 
 Then a regression tree is trained to predict these residuals, with each residual value placed in a leaf or terminal value of the tree according to feature values. Often there are more residuals than leaves, so for each leaf with multiple residuals in it, the average of the residuals is taken.
 
 After constructing the new decision tree, the model's prediction is updated. This is done by adding the tree's output values weighted by a learning rate $\nu$ (value between 0 and 1), to the initial prediction (the average). In other words, the model takes the previous prediction $F_{m-1}(x)$ and adjusts it using the corrections suggested by the new tree. \[F(x) = F_{m-1}(x) + \nu \sum^{J_m}_{j=1}\gamma_{jm}I(x\in R_{jm})\] $\sum^{J_m}_{j=1}\gamma_{jm}I(x\in R_{jm})$ just means add up the output values $\gamma_{jm}$ for all the leaves $R_{jm}$ that a sample $x$ can be found in.
  
\paragraph{Step 3: Making Predictions} \leavevmode

Once all $M$ trees have been built, the final model outputs the initial prediction plus the adjustments made by each tree. For new data, predictions are made by passing the input through all $M$ trees and summing their contributions: \[F_M(x) = F_0(x) + \text{corrections from 100 trees}\]

This process allows the model to gradually improve its accuracy, until it reaches an optimised state.

\subsection*{The LightGBM Algorithm}
LightGBM \cite{lightGBM} is a leaf-wise gradient boosting algorithm developed by Microsoft that focuses on fast performance speeds and efficiency over large datasets. It works by using a histogram-based technique which divides the data into discrete bins based on continuous values. As a result, instead of using the entire dataset to measure which split is the best, a bin can be used which approximates the whole dataset whilst using far less computational power and memory. The result is a much quicker process that still keeps a high accuracy.

LightGBM also uses leaf-wise tree growth instead of level-wise growth which means that instead of expanding all nodes at a given level in the tree before moving down to the next level, the node that reduces loss the most is expanded. This results in much deeper trees which are often more accurate, but also increases the risk of creating trees which overfit the data. The resulting trees are usually also harder to understand as they don't follow a balanced structure with the same number of nodes at each level. 

	% -------------------------------------------------------------
	% LEVEL WISE VS LEAF WISE
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.3]{leaf-vs-level}
	\caption{Level-wise and Leaf-wise decision tree growth \cite{lightGBM}.}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% LEVEL WISE VS LEAF WISE
	% -------------------------------------------------------------

Overall lightGBM has much faster training speeds and better efficiency when it comes to large and complex datasets. Its leaf-wise node expansion approach also allows for more complex and accurate decision trees than those built using normal gradient boosting methods.
% -------------------------------------------------------------
% METHODOLOGY
% ------------------------------------------------------------- 
\chapter[Methodology]{Methodology}
This chapter outlines the approaches taken towards developing the components that make up the final prediction model. The choices taken for the specific sentiment analysis and expected points models are outlined, including the model selection, data identification and processing, training and hyper-parameter tuning, as well as evaluation methods. The team selection algorithm is explained, and the manner in which these components combine to form and evaluate the final model are detailed.
% -------------------------------------------------------------
% OVERVIEW OF THE APPROACH
% -------------------------------------------------------------
\section[Overview of the Approach]{Overview of the Approach}
This project focuses on developing and evaluating a machine learning model for predicting FPL player performance using twitter sentiment and historical footballing data. The methodology consists of three main stages: 1 - creation of a sentiment analysis model, 2 - creation of an expected points prediction model, and 3 - model performance evaluation.

The first stage involves fine-tuning a sentiment analysis based BERT model to classify tweets about FPL players ahead of each gameweek. The resulting sentiment predictions for tweets are aggregated into sentiment scores for each player, then integrated into a dataset containing historical player performance data. Whilst existing FPL expected points models rely solely on structured historical data, this approach investigates whether incorporating Twitter sentiment can improve predictive performance.

In the second stage, a gradient boosting machine (GBM) model is trained on the combined dataset to estimate expected points for each player. The model’s output represents its prediction of how many FPL points a player is likely to score in the upcoming gameweek. The model's predictions for the final 8 gameweeks of the 2022/23 FPL season are saved as a dataset to evaluate the performance of the model.

Finally, the model's effectiveness is assessed by applying an algorithm that simulates FPL team selection to the prediction dataset. The team selection process is formulated as an optimisation problem, where integer linear programming is used to construct a fifteen-player squad and eleven-player starting team that maximises expected points whilst adhering to budget and constraint rules. This evaluation aims to quantify the impact of sentiment analysis on decision-making in FPL under real-world conditions.

By incorporating Twitter data into player performance predictions, this project explores the extent to which social media data can enhance existing FPL forecasting models. The findings may provide valuable insights for FPL managers seeking data-driven strategies to improve their decision-making.
% -------------------------------------------------------------
% SENTIMENT ANALYSIS MODEL
% -------------------------------------------------------------
\section[Sentiment Analysis Model]{Sentiment Analysis Model}
To capture crowd wisdom through Twitter, a sentiment analysis model is needed to automatically label tweets and gauge user's opinions towards each player. Whilst it is possible to train a sentiment analysis model from scratch, this study uses a pre-trained BERT model \cite{BERT} available for free from Google. This approach leverages access to a model that has been trained on billions of texts, giving it an extremely complex understanding of natural language. A model trained from scratch could not hope to replicate such complexity given the timeframe of this project. It would also be unreasonable to expect such a large task to be completed without the use of advanced computing machinery unavailable to individual researchers without the backing of a large corporation. The BERT model has been chosen specifically because it has been proven to outperform other state-of-the-art models like LSTM in sentiment analysis tasks\cite{elankath, hoang}.

This study uses fine-tuning to refine BERT for the specific task of sentiment analysis on FPL tweets. Fine tuning is the process of adapting a pre-trained model for specific tasks or use-cases \cite{fine-tuning}. This is a much less computationally demanding task than fully training a model, and only requires a relatively small dataset which is ideal for a project of this scale. By combining the extensive natural language knowledge that BERT already has with a suitable dataset of FPL tweets, a sophisticated and accurate sentiment analysis model that suits the needs of the project will be achieved.

\subsection[Data Collection]{Data Collection}
Identifying and collecting suitable data for fine-tuning the model is a critical step in creating a model with optimal performance. The quality of the fine-tuning data directly affects the model's output quality \cite{gigo}, so it was important to chose a dataset carefully.

The first approach considered was manual data collection directly via Twitter, however, exploring this approach led to many dead ends. The first problem was the financial cost of using the official Twitter API, which has seen a price increase from \$3 per month to \$100 per month since the company was sold in 2022 \cite{Twitter-costs}. This change has led to many free tools aimed at helping researchers leverage Twitter data for their studies being shut down, impacting the entire research field by limiting the options they have when it comes to collecting data. Without being able to use these tools, and with the official API unaffordable, using the API to efficiently collect data was not an option. Whilst other methods of collecting data exist, such as writing code scripts to scrape data from the Twitter website, or using 3rd party browser extensions, these methods are against the Twitter terms of service and could result in a permanent account ban if caught.

Manual collection of data on Twitter is an acceptable approach within the terms of service, however it is an extremely tedious process. When considering a dataset needed to fine tune a BERT model should contain thousands of entries, as well as the time constraints of this project, manually collecting and then labelling thousands of tweets was also not an option. 

The chosen approach was to use an existing dataset published on popular machine learning forums such as Hugging Face and Kaggle. These sites have thousands of active users contributing to a wide range of research topics. As such, finding a dataset for a similar purpose to this study's was not difficult. 4 potential datasets were identified on Kaggle, and include:

\begin{itemize}
    \item \href{https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets}{\textcolor{blue}{\uline{Fifa World Cup 2022 Tweets with Sentiment Labels}}} - (30k tweets)
    \item \href{https://www.kaggle.com/datasets/wjia26/epl-teams-Twitter-sentiment-dataset}{\textcolor{blue}{\uline{Premier League Teams Tweets with Sentiment Labels}}} - (460k tweets)
    \item \href{https://www.kaggle.com/datasets/prasad22/fpl-tweets-dataset}{\textcolor{blue}{\uline{FPL Tweets from 2012 - 2023, unlabelled}}} - (110k tweets)
    \item \href{https://www.kaggle.com/datasets/firefly55lm/tweets-footballers-premier-league}{\textcolor{blue}{\uline{Premier League Players Tweets with Sentiment Labels}}} - (167k tweets)
\end{itemize}

A further inspection of these 4 datasets was carried out to identify the most suitable given the needs of the model being built. 

The first dataset, whilst labelled, only contained tweets for the first day of the world cup. This meant a majority of tweets were centered around the event as a whole and the controversy of Qatar hosting, with little mention of players or actions they had carried out during football matches. The second dataset was much more suited to the needs of the model, however had the drawback of being centred around teams rather than individual players. The third dataset was focused specifically on FPL which appeared ideal at first. However it was found that this dataset contained a lot of unrelated tweets from people tweeting about their own performance in FPL, instead of expected performance of actual players.
	% -------------------------------------------------------------
	% NOISY DATA
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{fpl-tweets-dataset}
	\caption{Examples of noisy tweets irrelevant to player performance, such as personal FPL commentary}
	\label{default}
	\end{figure}
	% -------------------------------------------------------------
	% NOISY DATA
	% -------------------------------------------------------------

The fourth and final dataset was identified as the most suitable for this study. This dataset contains a far greater number of relevant tweets, focused on individual Premier League players from the 2022/23 season (it is almost impossible to find more recent data, due to the Twitter API price changes coming into effect from 2023). As well as the relevant focus of the tweets themselves, this dataset contains player labels for each row which is perfect for construction of an auxiliary sentence for target-based sentiment analysis. Instead of manually searching for players in each tweet, the existing \textit{player\_name} column in the dataset can be used. Finally, this dataset comes with the advantage of being almost fully cleaned of any noise including emojis, links, images, etc. reducing the amount of pre-processing required.

The sentiment labels provided in the dataset were automatically created by the Vader \cite{vader}, and TextBlob \cite{textblob} models. These models are fairly accurate however it is worth noting they do incorrectly label some tweets and therefore this dataset is not perfect. A potential solution to this problem is manually labelling the tweets, however this would require some criteria to match certain labels to tweets, and would also be extremely time consuming. Given the time constraints of this project and the limited human resources available for labelling, the dataset's size would have to decrease significantly to use a manual labelling approach. It is hoped that using the existing labels will be more beneficial due to the much larger dataset size available with this approach, which should result in a more accurate final model.
\newpage
% -------------------------------------------------------------
% DATA PRE-PROCESSING
% -------------------------------------------------------------
\subsection[Data Pre-Processing]{Data Pre-Processing}
The decision to cut the dataset down to a more suitable size of around 20k tweets was made, in order to keep training time reasonable (roughly 30 mins per epoch). Reducing the dataset size will most likely affect the model's performance negatively, however, given the time constraints of this project the computation time required to train the model on a dataset over 100k rows would have been too large.

Many tweets in the dataset mention multiple players, with some even containing conflicting sentiments e.g. "Haaland was poor today, massively outshone by Salah". In order to capture the separate sentiment values (target based sentiment) the method introduced by Sun et al. \cite{sun} was used, where an auxiliary sentence was created posing a question to the model. The chosen format was: "What do you think of the sentiment towards [player]?". Using this approach, the model inputs and outputs will look like:  

\begin{itemize}
    \item \textbf{Input:} What do you think of the sentiment towards Haaland?  
    \item \textbf{Output:} Negative
    \item \textbf{Input:} What do you think of the sentiment towards Salah?  
    \item \textbf{Output:} Positive
\end{itemize}

While a more FPL-specific auxiliary sentence (e.g., 'Should [player] be in my FPL squad?') was considered, it risked introducing noise due to sentiment labels from Vader/TextBlob. For example, 'Jamie Vardy has great hair!' might be labeled as 'yes' (suggesting he should be in an FPL squad) despite being unrelated to performance. Due to this potential issue, a more generic question was used with the intuition being that positive overall sentiment should closely correlate with positive FPL performance.
 
After adding the auxiliary sentence to the dataset, the rest of the tweets were cleaned of any noise. Removing noise from tweets ensures the model learns only relevant information, ultimately improving performance. Additionally, duplicates and empty rows were removed, and any other unnecessary columns were dropped. 

\subsection[Fine-Tuning BERT]{Fine-Tuning BERT}
To fine-tune a BERT model for sentiment analysis, the pre-trained BERT model can be modified by adding a task-specific classification layer on top of the BERT encoder stack \cite{BERT}. This final layer transforms the output weight values (represented as a vector $\mathbf{h}$) into a final class 'positive', 'negative', or 'neutral' sentiment by using probabilities to predict which class the model's output most closely represents.

\newpage
The classification layer is a softmax \cite{softmax} classifier, which takes as input a vector $\mathbf{h}$ and uses the following equation to predict the final class:
\[
p(c|\mathbf {h}) =\mathrm {softmax}(W\mathbf {h})
\]

The vector $\mathbf{h}$ corresponds to the token [CLS] which is a special token added to the input sentence before it is processed by the model. This token is modified as the input passes through the model’s layers, forming a condensed representation of the entire input sequence. By fine-tuning the model specifically for sentiment analysis, this token learns to focus on sentiment information. $W$ represents the task-specific weight matrix, which is updated during fine-tuning to minimise classification loss.
	% -------------------------------------------------------------
	% BERT FINE-TUNING
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.35]{sentiment-diagram}
	\caption{Single sentence classification \cite{sentiment-cls-diagram}}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% BERT FINE-TUNING
	% -------------------------------------------------------------

To minimise classification loss, a labeled dataset of texts and their corresponding sentiment labels (e.g., ‘positive,’ ‘negative,’ ‘neutral’) was used. During training, the model was fed these texts and its predictions were compared to the labels. Weights were updated iteratively to improve accuracy. This process was repeated for a set number of epochs (complete passes through the dataset) to improve accuracy whilst preventing overfitting.

In order to fine-tune BERT with the auxiliary sentence method outlined above, BERT is passed the auxiliary sentence and the original tweet as a sentence pair, with a [SEP] (separator) token placed in between them. Classification is done in the same way as single sentence sentiment analysis with the hidden vector $\mathbf{h}$ used to predict the label.

	% -------------------------------------------------------------
	% BERT FINE-TUNING
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.35]{sentence-pair-cls-diagram}
	\caption{Sentence-pair classification \cite{sentence-pair-cls-diagram}}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% BERT FINE-TUNING
	% -------------------------------------------------------------

The fine-tuning process not only adapts BERT to specific targets but also allows it to capture domain-specific language features that differ from general usage. For instance, while the word “soft” might generally convey positive sentiment (e.g., a soft blanket), in the FPL context, calling a player "soft" is often a negative sentiment. By fine-tuning BERT on domain-specific tweets, the model can learn such nuanced language patterns, improving its accuracy in the target domain.

Fine-tuning BERT is computationally efficient compared to training a model from scratch and requires a relatively small amount of labeled data. As such it was a practical approach that suited the needs of this project given the time restraints.
\newpage
% -------------------------------------------------------------
% HYPERPARAMETER OPTIMISATION
% -------------------------------------------------------------
\subsection[Hyper-parameter Optimisation]{Hyper-parameter Optimisation}
\label{sec:bert-hp}
In order to maximise the performance of the final model, hyper-parameters were optimised. Selecting a suitable set of hyper-parameters is crucial for model performance, and adjusting these values during training can help the model to avoid overfitting the training data, minimise validation loss, and maximise accuracy \cite{hyperparameters}. Typically when implementing a machine learning model, researchers manually tweak hyper-parameters and spend significant time exploring configurations that may not improve performance meaningfully. This manual process is inefficient and resource-intensive, so by using an automatic framework, less computational resources and time is spent developing the model.

The framework chosen was Optuna \cite{optuna}, an automatic hyper-parameter optimisation framework for Python that finds the most optimal hyper-parameter values via trial and error. It conducts a series of trials and uses the previous trial to identify promising potential tweaks that could be made to improve performance. This process is repeated and a history of trials is kept throughout the process to guide the next changes made to the hyper-parameters. Through enough trial and error, optimal hyper-parameter values are found. 

The search space for hyper-parameters was defined based on the values recommended in the original BERT paper \cite{BERT}. The chosen hyper-parameters along with their respective search space were:

\begin{itemize}
	\item Learning Rate ($2 \times 10^{-5}$ to $5 \times 10^{-5}$) - determines the step size at which the model updates its weights during training. A higher learning rate can speed up training time but risks overshooting the optimal parameters, whilst a lower learning rate ensures stability but may result in slower training.
	\item Batch Size (16 or 32) - Batch size refers to the number of training samples processed simultaneously before updating the model's weights. A smaller batch size can introduce noise in the gradient updates, which sometimes aids in escaping local minima but can lead to slower convergence. A larger batch size offers smoother gradient updates but may require more computational resources and risks overfitting.
	\item Number of Epochs (2 to 4) - specifies how many times the entire training dataset will be processed by the model during fine-tuning. Too many epochs can cause the model to focus on specific patterns in the training data, leading to overfitting. Too few epochs can limit the model's ability to learn from the data and lead to poor accuracy.
\end{itemize}

The datasets used during this stage were slimmed down to reduce training time. Evaluating models trained with the entire dataset would have taken too long (90 mins per trial) and used too much computing resource, so instead a dataset 25\% of the size was used. It is hoped this approach resulted in a decent general view of the best configurations, of which a small amount were then explored further using the whole dataset.
% -------------------------------------------------------------
% MODEL EVALUATION
% -------------------------------------------------------------
\subsection[Model Evaluation]{Model Evaluation}
In order to ensure the best configuration was chosen, the three best configurations discovered during hyper-parameter optimisation were used to fully train 3 models.

These models were evaluated using the following performance metrics to ensure the best model was chosen:

\begin{itemize} 
	\item \textbf{Accuracy} - The percentage of predictions that the model correctly makes across all classes. 
	\item \textbf{Precision} - The combined percentage of predictions that are correct for each class, weighted by the number of instances in each class. E.g. if 30 samples are labelled positive, and the model correctly predicts them all, but also incorrectly predicts 10 more samples as positive the precision for that class will be $30/40 = 75\%$. 
	\item \textbf{Recall} - The combined percentage of true instances that were correctly identified by the model, also weighted by the number of instances in each class. For the same example where all 30 positive samples were correctly predicted, the recall would be 100\% for the positive class.
	\item \textbf{F1 Score} - The harmonic mean of precision and recall, calculated as: 
 \[
F_1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
\]
\end{itemize}

Using the F1 score as a performance metric should reward models that are both precise and able to recall relevant rows of data effectively, ensuring a robust model. F1 score is particularly relevant for sentiment analysis with multiple labels, as it ensures a model is not just performing well on the majority class, and can accurately predict less frequent classes of data.

The three models were evaluated on approximately 1000 samples taken randomly from the unseen data, ensuring no prior knowledge from training could influence the evaluation, and providing a fair assessment of the models' performance. The unseen data was processed in the exact same way as the training data to ensure consistency.

After evaluation of the three models, the best model was selected to be used to create the enhanced dataset to train the expected points model.
\newpage
% -------------------------------------------------------------
% EXPECTED POINTS MODEL
% -------------------------------------------------------------
\section[Expected Points Model]{Expected Points Model}
In order to identify the best team of players ahead of an FPL gameweek, a prediction model that can accurately forecast the number of points each player is going to score was developed. Predicting player performance in Fantasy Premier League is a well-explored problem. Many studies have used a variety of machine learning and statistical methods in order to achieve this goal. The study most similar to this one, carried out by Bonello et al. \cite{bonello} in which news articles and betting markets were used to improve traditional model performance, found that Gradient Boosting Machines (GBMs) were the most effective. They state "ensemble methods show[ed] far higher suitability to the task when compared with alternate supervised learning approaches such as SVMs." 

Due to the similar nature of this study (the main difference is the source of crowd wisdom coming from Twitter instead of news articles and betting markets) GBMs were chosen as the approach for expected points model. In selecting GBMs, this study builds on the success of previous studies where they have been proven to handle complex datasets with lots of interacting features. The selection of Twitter data adds a unique advantage, capturing opinions from millions of fans around the world instead of just 'expert' journalists. As well as this, FPL-specific information such as when a double gameweek is coming up for certain teams will typically not be reported by traditional news outlets.
% -------------------------------------------------------------
% DATA COLLECTION AND ENGINEERING
% -------------------------------------------------------------
\subsection[Data Collection and Engineering]{Data Collection and Engineering}
The core of the dataset containing historical information for each player was taken from a community-run \href{https://github.com/vaastav/Fantasy-Premier-League/tree/master}{\textcolor{blue}{\uline{Github repo}}} set up by Anand \cite{anand}. This repository contains data for each FPL season dating back to 2016, including information for every player and every gameweek. This dataset was used by GS when he created his binary classification model in 2018 \cite{raghunandh}, so it made sense to combine this approach with the sentiment data collected, in order to get a fair comparison. Data for the 2022/23 FPL season was used, as this matches the timeline of the most recent Twitter data available (more recent data locked behind \$100 per month API).

As well as the raw data available, rolling features were computed such as 'average\_points\_last\_3\_weeks', and 'minutes\_last\_3\_weeks', in order to capture a greater range information about each player ahead of a gameweek. In total 21 input parameters were chosen, with roughly 3 main categories for the type of information captured by each input: 

\begin{itemize}
    \item \textbf{Information known before kick-off} - e.g., home or away, opponent difficulty
    \item \textbf{Player specific data} - e.g., average points last 3 weeks, goals scored last 3 weeks
    \item \textbf{Team specific data} - e.g., team form, team goals scored last 3 weeks
\end{itemize}

To incorporate sentiment data into the dataset, unseen Twitter data was run through the sentiment analysis model to get a sentiment value for each row in the dataset. Each tweet was then tagged with the FPL gameweek it belonged to by taking start dates for each gameweek and matching tweets to the closest start date.

This process resulted in a dataset of unseen tweets with a corresponding sentiment label, for each player and gameweek. These tweets were matched to their corresponding player and gameweek from the base dataset. For each match, two new columns, containing the number of positive and negative tweets for each player were added to the dataset, resulting in the final dataset which included sentiment information.

\subsection[Data Pre-Processing]{Data Pre-Processing}
To pre-process the dataset before training the model, each numerical value was normalised (scaled to a value between 1 and 0). This was done so all features in the dataset had the same scale, which is important for the convergence of GBMs. Normalisation was performed using Min-Max scaling, which rescales each feature by subtracting the minimum value and dividing by the range (maximum value - minimum value). This ensures that no feature dominates the learning process due to differing scales, and that the model can more efficiently optimise during training.

After this any rows with empty values or duplicate rows were dropped from the dataset completely. Then the dataset was split into train and test datasets, with an additional evaluation set kept back for final evaluation of the model. The approach taken for this was to keep back data for the last 8 gameweeks of the season (gameweeks 31-38) so that the final model could be evaluated for a consecutive 8 weeks in a row. With more data available, it would have been better to test the model over the course of a whole season, or even in real time to see how it copes with the most up to date information. However, as mentioned many times previously, the limited amount of twitter data available for free makes this impossible to do. There may be some bias in the data with certain events like double gameweeks happening at the end of the season, but this was chosen as the best approach with the limited amount of data available.
% -------------------------------------------------------------
% MODEL TRAINING AND EVALUATION
% -------------------------------------------------------------
\subsection[Model Training and Evaluation]{Model Training and Evaluation}
\label{sec:gbm-training}
A Gradient Boosting Machine (GBM) model was trained to predict expected points for each player. The train and test datasets were stratified to ensure an even distribution of data from different gameweeks. Making sure data was evenly distributed was important to avoid model bias towards a particular period of the season, improving the model's ability to generalise.

Training of a GBM is slightly different to a typical neural network like the one used in the final layer of the BERT model. A GBM works by building a 'tree' (a simple decision-making structure) that initially outputs the mean of the target values, and then calculates the residual errors (difference between the model's prediction and the true values) to build a new better tree. Instead of updating model weights like a neural network, new trees are added to reduce the loss progressively. For a more detailed explanation of decision trees and GBMs, see chapter 3.

An initial GBM model was trained with default hyper-parameters, and the resulting performance was checked to make sure there were no obvious errors or unexpected results. To find the optimal hyper-parameters for the GBM model, Optuna \cite{optuna} was once again used just as for the sentiment analysis model. Root Mean Squared Error (RMSE) was chosen as the evaluation metric, as this directly quantifies the accuracy of points predictions. Due to the much faster training speeds seen when training a GBM as compared to a BERT model, 50 different combinations of hyper-parameters were explored. Early stopping was implemented to prevent overfitting so that iff the validation loss did not improve for 10 consecutive iterations, training was halted. This was to speed up the training process and reduce computational demand.

The hyper-parameter space was defined as: 

\begin{itemize}
	\item Number of Leaves (20 - 100) - controls the maximum number of leaves per tree (complexity). Higher complexity can increase performance but leaves the model susceptible to overfitting. 
	\item Learning Rate (0.01 - 0.1) - controls the step size during gradient boosting. Lower learning rate leads to slower convergence but better generalisation, while higher learning rate leads to faster convergence but risks overshooting.
	\item Min Data in Leaf (100 - 1000) - controls minimum number of data points in a leaf. Lower values risk overfitting, whilst higher values capture more information per leaf so are better at generalising.
	\item Max Depth (3 - 15) - controls the maximum depth of each tree. Deeper trees are more complex but risk overfitting
\end{itemize}

The best-performing set of hyper-parameters was selected based and was used to train the final model. An additional model trained with no sentiment data was also saved, so that a comparison in performance could be made and any differences between the two models could be observed. The hope being the model trained with sentiment data would show some improvements to reflect the additional data made available to it.

Finally, the final evaluation data was then run through the model to get expected points predictions for each entry in the dataset. This resulting dataset containing model predictions and actual points scored for the last 8 gameweeks of the season was used for model evaluation with the team selection algorithm.
% -------------------------------------------------------------
% OPTIMAL TEAM SELECTION ALGORITHM
% -------------------------------------------------------------
\section[Optimal Team Selection Algorithm]{Optimal Team Selection Algorithm}
To fully evaluate the performance of the final prediction model, it needs to be tested under real-world conditions. This means using unseen data to gather predictions for real FPL gameweeks. This prediction data needs to be transformed from a points prediction score for each player in the game, to a useful team recommendation. 

In order to do this and form real teams that satisfy the rules of the game, a selection algorithm was constructed to find the best possible team from the information available. This selection algorithm used integer linear programming in order to select the most optimal 15 players (maximise expected points) whilst satisfying the budgetary, positional, and squad size constraints. The pool of available players contained around 500 individuals making the optimisation computationally significant and non-trivial.

\subsection[Constraints]{Constraints}
Detailed in section~\ref{sec:rules-of-fpl}, the rules of FPL place many constraints on how a squad of players can be formed. The constraints placed on an initial squad formation are as follows:

\begin{itemize} 
	\item \textbf{Squad Size} - The number of players in a squad must be equal to 15. 
	\item \textbf{Squad Value} - The total cost of all players in a squad must not be more than £100M.
	\item \textbf{Positional Constraints} - A squad must be made up of 2 goalkeepers, 5 defenders, 5 midfielders, and 3 strikers.
	\item \textbf{Team Constraints} - A squad cannot have more than 3 players belonging to the same real-world premier league team
\end{itemize}

As well as selecting an initial squad of 15, the algorithm needs to select a 'Starting XI' whose points will count towards the final score each gameweek. Positional constraints placed on the 11 are:

\begin{itemize} 
	\item \textbf{One Goalkeeper}
	\item \textbf{Between Three and Five Defenders} 
	\item \textbf{Between Three and Five midfielders} 	
	\item \textbf{Between One Three Strikers} 
\end{itemize}

As such, after running data through the team selection algorithm to get the best 15 players, an additional function to pick the best 11 players from a squad of 15 (fitting the positional constraints) was constructed. 

\subsection[Algorithm]{Algorithm}
The algorithm uses linear integer programming which is a mathematical optimisation technique. For the context of FPL team selection, the problem is defined with each player from the available pool of ~500 being assigned a decision variable (set to 1 if player selected, 0 if not). An objective function is then defined which is to maximise the sum of the expected points for all players selected. 

Additionally, the constraints mentioned above are modelled as a linear set of rules for the optimiser to follow when solving the problem. The PuLP library is utilised to explore all possible solutions in an effective way using branch-and-bound techniques. If a solution is possible, the PuLP solver class will output the optimal solution after finishing the search through the solution space.

Once the squad has been selected, the problem of choosing a starting eleven is simple. Initially the minimum amount of players in each position are chosen based on points - e.g. best GK, 3 best DEFs, 3 best MIDs and best ST. Then the final 3 players are chosen by taking the 3 players with the highest expected points, completing the eleven.
\subsection[Final Evaluation with Selection Algorithm]{Final Evaluation with Selection Algorithm}
With the team selection algorithm in place, a set of teams for each gameweek (31-38) and each model (all data, no sentiment data, and true values) were created. 

To do this the dataset created from the predictions of the final GBM models was split by gameweek, resulting in 8 gameweeks of data. Then the expected points predictions for the no sentiment model, the all data model, and the actual points scored were extracted. The data for each model and gameweek was then run through the team selection algorithm to get a squad, starting eleven, and total points scored.

For an accurate comparison against baselines like the average manager score, the captaincy was also simulated. The captain chip is used by managers to pick one player from their starting eleven whose points are doubled. In this case the player with the highest expected points had their actual points doubled to simulate them being chosen as the captain.

The resulting data for each gameweek and model was used for the final evaluation of the model.
% -------------------------------------------------------------
% SUMMARY
% -------------------------------------------------------------
\section[Summary]{Summary}
To summarise, this chapter has outlined the three main components that have been developed in order to create and evaluate the final prediction model. First, the sentiment analysis BERT model was fine-tuned to classify FPL related tweets, generating aggregated sentiment scores for each player. Secondly, a gradient boosting machine (GBM) model was trained on historical footballing data combined with the aggregated sentiment scores to predict how many points each player would score in an upcoming gameweek. Finally an integer linear programming algorithm was implemented to select an optimal team based on expected points predictions, simulating real world FPL decision making under the constraints of the game. 

Together these components make up a complete pipeline for FPL decision making based on historical and Twitter-based sentiment data. The results and evaluation of the final model are detailed in chapter~\ref{chapter:results}.
\chapter[Implementation]{Implementation}
% -------------------------------------------------------------
% IMPLEMENTATION
% -------------------------------------------------------------
This chapter describes the technical implementation of the FPL decision making model and evaluation pipeline, providing a comprehensive overview of how each component was created. It details the specific tools, libraries, and APIs used throughout the project such as Hugging Face Transformers for sentiment analysis, LightGBM for points prediction, and PuLP for team selection. Each stage of the development pipeline is explained, with steps taken for data collection, preprocessing, model training, and hyperparameter tuning included.

On top of this, this chapter includes details of how each component was combined to form a cohesive and well-structured pipeline. It also explains how different evaluation metrics were used to assess model performance at intermediate stages throughout the development process. The techniques used to improve these metrics along with considerations such as bias-variance tradeoffs are explained, detailing how imbalanced data and generalisation across gameweeks was handled. The goal of this chapter is to provide a clear and reproducible account of how the theoretical methodology was converted into a working decision making model.
% -------------------------------------------------------------
% SENTIMENT ANALYSIS MODEL
% -------------------------------------------------------------
\section[Sentiment Analysis Model]{Sentiment Analysis Model}
The BERT model used in this study to predict sentiment for FPL players was fine-tuned in Google Colab \cite{colab} using Python. Google Colab is a cloud computing platform that was chosen as it allows free GPU and TPU access, much better suited to fine-tuning a deep learning model than a CPU found in a typical laptop - helping reduce training time. Its Jupyter notebook format also allows for simple and clear separation of functions with markup text support, allowing for single blocks of code to be independently executed and described with headings and text. This allowed for easy debugging and troubleshooting of code, improving the development experience. Multiple Colab notebooks were set up to create a pipeline for pre-processing data, model training, hyper-parameter optimisation, and evaluation of the model. This format allowed for easy and quick reproduction of results, making it easy to tweak parts of code and try new approaches.
 
Python was chosen as the preferred language due to the extensive number of libraries that exist supporting the development of deep learning models. Some of the most popular include Hugging Face's Transformers library \cite{huggingface-transformers} which contains APIs for hundreds of pre-trained transformer models including BERT. The transformers library also includes tokenisers for each model and a PyTorch \cite{pytorch} Trainer class that allows fine-tuning to be done with just a few lines of code. The model's hyper-parameters can be fine tuned and optimised using the Optuna library \cite{optuna} to explore the best possible combinations. Use of these libraries ensure correct approaches are taken, and massively reduces development time by simplifying the entire process.

\subsection[Data Pre-Processing]{Data Pre-Processing}
To start pre-processing the dataset, as much noise as possible was removed. Any image links were identified and removed by searching for occurrences of 'a href' in the texts. Empty rows and duplicates were stripped out, and regular expressions were used to check if the player label for each tweet (which player the tweet is supposedly about) is actually mentioned in the text. 

For example, some entries in the dataset contained player labels such as 'Jordan Pickford' but when inspecting the tweet content, no mention of him was found. To ensure a consistent format and to remove noise likely to disrupt model training, these entries were removed from the dataset.

The auxiliary sentence needed for the target-based sentiment analysis approach outlined in the methodology was then created for each entry in the dataset. This was done by using the \texttt{player\_name} column to construct a sentence in the form "What do you think of the sentiment towards [\texttt{player\_name}]". The column names were adjusted and reordered so that the final dataset contained just three rows: 'question', 'context', and 'label'.

Finally, all the text data was converted into numerical format so the model could understand via tokenisation. To do this the \texttt{AutoTokenizer} class from Hugging Face's Transformers library was used to load in the standard BERT tokeniser. The columns were tokenised and then saved. The 'label' class was mapped to integers, with 0 representing positive, 1 representing negative, and 2 representing neutral. 

Once this was done the dataset was split into train and validation sets using the \texttt{train\_test\_split} method from scikitlearn with a \texttt{test\_size} of 0.1 and \texttt{seed} set to 42. The datasets were then ready to be used for training of the model.
\newpage

\subsection[Training the Model]{Training the Model}
The next step in the pipeline was to use the processed datasets to train the model's classification layer used to fine-tune it. The first step of training was to configure the \texttt{TrainingArguments} from the Transformers library. This class contains the hyper-parameters for the model such as learning rate, batch size, epochs, and weight decay. These hyper-parameters can be tweaked to improve model performance and prevent overfitting during training. Initially, these were set to default values suggested by the Hugging Face tutorial, with the intention of optimising them later (see~\ref{sec:bert-hp-optimisation}).  
	
After these arguments were configured, the Hugging Face \texttt{Trainer} class was set up. To do this the pre-trained BERT model was loaded in from the transformers library, with the \texttt{BertForSequenceClassification} variant used - because a sequence of text (the auxiliary sentence and tweet content) was being used to classify tweets. The specific model used was \texttt{'bert-base-cased'} which is the case-sensitive base variant of the BERT model. The case sensitive variant was used due to tweets often containing capitalisation to convey extreme sentiment (e.g. 'COME ON WHAT A GOAL!!'). The \texttt{Trainer} class was initialised by passing in the model, train and validation datasets, and \texttt{TrainingArguments}. Once the Trainer had been initialised, the training process was started by calling the \texttt{train()} method.

Training took around ninety minutes, and the resulting model scored an initial validation accuracy of around 90.34\%. Other performance metrics such as precision and F1 score were configured and will be discussed further during final model evaluation. Focusing on the validation loss, it appeared that the initial model was starting to overfit the training data as the epochs increased, evidenced by the increasing in value after epoch 2. 
	% -------------------------------------------------------------
	% INTIAL METRICS
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.46]{initial-train}
	\caption{Performance metrics for the initial model}
	\label{default}
	\end{figure}
	% -------------------------------------------------------------
	% INITIAL METRICS
	% -------------------------------------------------------------

Once trained, the initial model was tested to ensure that it produced outputs as expected. 5 samples from the unseen data were taken and passed through the model to infer an output. The unseen data was processed by the same tokeniser function used for the training data to ensure it was pre-processed in the exact same way as the training data. This was important for achieving an accurate prediction from the model as it ensured the model recognised the inputs and could leverage knowledge learned during training to the fullest extent. Inference was done using the built in \texttt{eval()} method for the \texttt{model} class provided by the Transformers library.
	% -------------------------------------------------------------
	% INFERENCE
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.35]{inference}
	\caption{The model's predictions on unseen data}
	\label{default}
	\end{figure}
	% -------------------------------------------------------------
	% INFERENCE
	% -------------------------------------------------------------

Model outputs appeared to be reasonable, and were easily retrieved through the built-in methods provided by the transformers library. This validated that generating a list of sentiment predictions for a large set of tweets would be feasible, which was necessary for building the final model. 

\subsection[Hyper-parameter Optimisation]{Hyper-parameter Optimisation}
\label{sec:bert-hp-optimisation}
Optuna was the chosen library for tuning hyper-parameters, due to its extensive documentation and computationally efficient approach. 

The first step in configuring Optuna was to specify the hyper-parameters to be tweaked and the space to be explored. To do this, an objective function was defined to guide the hyper-parameter tuning process. This function evaluates the performance of each 'trial' which is a model trained on a set of hyper-parameters suggested by Optuna. 

Firstly the search space and hyper-parameters available were defined following the ranges specified in~\ref{sec:bert-hp}, for Optuna's to generate suggestions for the model in each trial. The metric used to evaluate models was also defined, with validation loss being used in this case. The validation loss is a measure of how far away the model's predictions are from the true labelled values of the validation dataset. To achieve an accurate model, this metric should be as low as possible to indicate the model's guesses are very close to the truth, thus the goal of the Optuna \texttt{study} was set to minimise validation loss.

An Optuna study was created using the \texttt{create\_study} method, and then the \texttt{optimize} method was called, with the objective function passed and \texttt{n\_trials} set to 10. This number of trials should allow Optuna to explore a sufficient amount of configurations while keeping resource usage to an acceptable level. Because Optuna chooses each subsequent configuration based on the previous one, each new configuration should explore meaningful search space and therefore enable a near optimal configuration to be found in 10 trials.

The configurations and resulting validation loss for each trial were:

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Trial} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{Epochs} & \textbf{Validation Loss} \\ \hline
1 & $3.895 \times 10^{-5}$ & 16 & 3 & 0.626 \\ \hline
2 & $3.996 \times 10^{-5}$ & 16 & 4 & 0.629 \\ \hline
3 & $3.946 \times 10^{-5}$ & 32 & 3 & 0.783 \\ \hline
4 & $2.464 \times 10^{-5}$ & 32 & 3 & 0.847 \\ \hline
5 & $4.504 \times 10^{-5}$ & 16 & 3 & \textbf{0.524} \\ \hline
6 & $2.609 \times 10^{-5}$ & 16 & 2 & 0.876 \\ \hline
7 & $3.742 \times 10^{-5}$& 32 & 2 & 0.890 \\ \hline
8 & $2.007 \times 10^{-5}$ & 16 & 2 & 0.844 \\ \hline
9 & $3.494 \times 10^{-5}$ & 32 & 3 & 0.736 \\ \hline
10 & $3.710 \times 10^{-5}$ & 16 & 3 & 0.705 \\ \hline
\end{tabular}
\caption{Hyper-parameter configurations with associated validation loss}
\label{tab:scoring_system}
\end{table}


\newpage
\subsection[Model Evaluation]{Model Evaluation}
Whilst fully training the three best configurations, it was initially discovered that these configurations started to overfit due to an increase in data samples, so to compensate for this training was reduced by one epoch. Although optimising the hyper-parameters with the full dataset would give the best results, the time taken to do this (around 15 hours for 10 trials) would have been too large, so this approach was chosen as a compromise.

The configurations evaluated were the best 3 from hyper-parameter optimisation: Trial 5 (Configuration 1), Trial 1 (Configuration 2), and Trial 2 (Configuration 3). The results are as follows:

\begin{table}[ht]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Class        & Precision & Recall & F1-Score & Samples \\ 
        \midrule
        Positive     & 0.95      & 0.75   & 0.84     & 483     \\ 
        Negative     & 0.86      & 0.62   & 0.72     & 190     \\ 
        Neutral      & 0.71      & 0.99   & 0.83     & 402     \\ 
        \midrule
        Accuracy     & \multicolumn{3}{c}{0.82} & 1075 \\  
        Weighted Avg & 0.85      & 0.82   & 0.81     & 1075     \\ 
        \bottomrule
    \end{tabular}
    \caption{Classification report for Model Configuration 1.}
    \label{tab:config1}
\end{table}
\begin{table}[ht]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Class        & Precision & Recall & F1-Score & Samples \\ 
        \midrule
        Positive     & 0.96      & 0.44   & 0.61     & 483     \\ 
        Negative     & 0.87      & 0.32   & 0.46     & 190     \\ 
        Neutral      & 0.51      & 0.99   & 0.67     & 402     \\ 
        \midrule
        Accuracy     & \multicolumn{3}{c}{0.63} & 1075 \\ 
        Weighted Avg & 0.77      & 0.63   & 0.61     & 1075     \\ 
        \bottomrule
    \end{tabular}
    \caption{Classification report for Model Configuration 2.}
    \label{tab:config1}
\end{table}
\begin{table}[ht]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Class        & Precision & Recall & F1-Score & Samples \\ 
        \midrule
        Positive     & 0.94     & 0.55   & 0.69    & 483     \\ 
        Negative     & 0.81      & 0.59   & 0.69     & 190     \\ 
        Neutral      & 0.61      & 0.99   & 0.76     & 402     \\ 
        \midrule
        Accuracy     & \multicolumn{3}{c}{0.72} & 1075 \\ 
        Weighted Avg & 0.79     & 0.72   & 0.72     & 1075     \\ 
        \bottomrule
    \end{tabular}
    \caption{Classification report for Model Configuration 3.}
    \label{tab:config1}
\end{table}

\newpage

The results suggest that the models struggled with the negative class in all configurations. This was expected due to the class imbalance present in the dataset, which contains far fewer "negative" samples compared to the "neutral" and "positive" samples. The model seems to mislabel "negative" samples as "neutral" due to the higher number of "neutral" samples during training. This is reflected in the lower precision and recall for the negative class across all configurations.

The neutral class has high recall (close to 1.00) but suffers from lower precision in some configurations, particularly in Configuration 2, where the model incorrectly classifies a significant portion of the negative samples as neutral. The positive class also shows a reasonably balanced performance, but it is clear that the class imbalance impacts the model's performance, as seen in the discrepancies between recall and precision for the "negative" and "neutral" classes.

After evaluating the models, Configuration 1 (Trial 5) stood out as the best model based on the F1-score, which is a better reflection of the model’s overall ability to balance precision and recall. The F1-score of Configuration 1 was significantly higher than the other configurations, suggesting it was the most reliable model for classifying positive, negative, and neutral sentiments.

As such, the model trained using Configuration 1 was saved and uploaded to a newly created Hugging Face repository using the API provided. From there it was easily accessible to all other components of this project.
% -------------------------------------------------------------
% EXPECTED POINTS MODEL	
% -------------------------------------------------------------
\section[Expected Points Model]{Expected Points Model}
The GBM model was implemented using the LightGBM library \cite{lightGBM} due to its comprehensive documentation, simple APIs, and impressive performance aided by the use of GPU learning. LightGBM is a gradient boosting framework that uses a leaf-wise tree splitting approach rather than a level-wise one, which improves overall model performance as more complex trees can be created. More complex trees can however lead to overfitting, so ensuring that the model hyper-parameters are configured correctly is an even more important task than it would be for frameworks using a different algorithm. LightGBM has been shown to outperform other popular algorithms such as XGBoost, making it an appropriate choice for this task \cite{lightGBM-vs-XGBoost}.

Two more Google Colab notebooks were created, one to facilitate the creation of the dataset combining data from the sentiment analysis model and historical footballing data, and a further to create the model training and evaluation pipeline. Again, Python was chosen as the development language due to its wide range of machine learning libraries such as Pandas and Numpy. Additionally, Matplotlib was used to create visualisations for the final evaluation of the models with and without sentiment data.

\subsection[Dataset Creation and Feature Engineering]{Dataset Creation and Feature Engineering}
In order to train the GBM model, a dataset was created with the most relevant features possible. The first stage in achieving this was downloading the historical dataset from the \href{https://github.com/vaastav/Fantasy-Premier-League/tree/master}{\textcolor{blue}{\uline{Github repo}}}. This was a simple case of downloading the raw files from Github for each gameweek of the 2022/23 season via GET request, and saving them.

From this point the 'rolling' features were computed, such as 'starts\_last\_3\_weeks' and 'points\_last\_3\_weeks', by using the inbuilt \texttt{.shift} and \texttt{.rolling} methods from Pandas. The dataset rows were grouped by name, team, position and gameweek to obtain unique records. From here \texttt{.shift(1)} could be used to look back 1 gameweek, whilst \texttt{.shift(1).rolling(3)} would look back at 3 previous gameweeks. This allowed for easy and efficient calculation of features, without having to use any loops or array methods. Once these calculations were done, any empty rows were dropped and the relevant features were saved to a dataframe. 

The list of relevant features used for the dataset is as follows:

 \texttt{'points\_last\_week', 'points\_2\_weeks\_ago', 'points\_3\_weeks\_ago',\\
 'avg\_points\_last\_3\_weeks', 'expected\_goal\_involvements\_last\_3\_weeks', 'ict\_index\_last\_3\_weeks','creativity\_last\_3\_weeks',\\
 'threat\_last\_3\_weeks', 'bps\_last\_3\_weeks', 'minutes\_last\_week',\\
 'minutes\_last\_3\_weeks','starts\_last\_3\_weeks',\\
 'transfers\_in\_last\_week', 'transfers\_out\_last\_week',\\
 'value\_change\_last\_week', 'was\_home\_last\_week',\\
 'team\_goals\_scored\_last\_3\_weeks','team\_clean\_sheets\_last\_3\_weeks',\\
 'total\_points', 'value'}

 After saving these features the dataset was examined to ensure features had been correctly calculated. 
 
 	% -------------------------------------------------------------
	% DATASET VIEW
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.38]{historical-data}
	\caption{The dataset with rolling features computed.}
	\label{default}
	\end{figure}
	% -------------------------------------------------------------
	% DATASET VIEW
	% -------------------------------------------------------------

The next step was to add in the sentiment data. This was done by taking the unseen Twitter data from the dataset identified in the sentiment analysis chapter, and running those tweets through the sentiment analysis model to get a list of labelled tweets for every player and every gameweek. To do this each tweet from the dataset was tagged with the gameweek it belonged to, using the tweet timestamp. The start date of each gameweek was taken from the same Github repo that contains the historical data, so matching the start dates with the tweet timestamp was trivial. 

Next the data was pre-processed using the same steps as model training, and then the saved sentiment analysis model was loaded using the Hugging Face API so the tweets could be run through it to get a sentiment prediction. 

 	% -------------------------------------------------------------
	% LABELLED TWEETS
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.48]{labelled-tweets}
	\caption{The tweets after being run through the sentiment analysis model.}
	\label{default}
	\end{figure}
	% -------------------------------------------------------------
	% LABELLED TWEETS
	% -------------------------------------------------------------
	
The positive and negative sentiment values for each player for each gameweek were then summed, resulting in a 'positive\_tweets' and 'negative\_tweets' counter for each player and gameweek. The player and gameweek columns were then matched with the same record in the dataframe containing historical information, and merge was performed to get a final dataset. 

When implementing name matching, it became clear there were some player names that contained typos and abnormal characters (see Figure 5.4) so matching player names was not behaving as expected. To fix this, the \texttt{rapidfuzz} library was used, which is a matching library that accounts for typos. When checking if two names are the same, it creates a confidence score based on how similar the texts are. To account for typos and missing characters a confidence threshold of 80\% was set. The inclusion of this matching approach meant the sentiment data was correctly appended to the main dataframe.

This final dataframe was saved as a CSV file and then uploaded to a Hugging Face repository so that it could be used by the second notebook for training and evaluating the GBM model.
\newpage 

\subsection[Data Pre-Processing]{Data Pre-Processing}
To process the dataset before it was used for training, sklearn and Pandas were used. Firstly empty rows and duplicates were removed from the dataset using the \texttt{dropna} and \texttt{drop\_duplicates} methods belonging to the Pandas dataframe.

Next the data was split into training and validation sets, but first the data from gameweeks 31-38 was kept back for a final evalution dataset. The remaining gameweeks' data was split into train and validation sets using the \texttt{train\_test\_split} method from sklearn. The \texttt{test\_size} parameter was set to 0.15. The purpose of the final evaluation dataset was to keep hidden data, so the final model could be fairly evaluated without any data leakage during training. The test and train set were used as normal for model training, with around 15k training samples and 3k test samples. 

Inputs were scaled using the \texttt{MinMaxScaler} from sklearn which is a simple way of normalising the input values between 0 and 1 to prevent large values from one input being overrepresented by the model weights during training. A different scaler was used for inputs and the target variable 'total\_points' so that the predictions could be unscaled at the end. The scaler was fit on the training dataset and then used to scale the final evaluation and validation sets. This way no data from evaluation and validation sets influenced the scaler values.

Finally pandas was used to remove the identifying features: name, gameweek, position, and value from the actual features, with the intent to add them back to the dataset once final model predictions had been generated. This way a dataset of expected points for each player and gameweek could be generated, with enough information to enforce team selection constraints to form a team.

\subsection[Model Training and Hyperparameter Optimisation]{Model Training and Hyperparameter Optimisation}
Initially the model was trained with default hyper-parameters, using the class \texttt{LGBMRegressor} from LightGBM. This class makes it extremely simple to create a regression model and train it, perfect for the needs of this project. A model object of the class was instantiated passing in 'rsme' as the metric, and then the \texttt{.fit} method was called passing in the training data inputs and the target variable.

The \texttt{.predict} method was called on the same model with the validation data to get a performance overview of the model. To get the metrics sklearn and numpy were used, calculating the Root Mean Squared Error (RMSE) using the \texttt{sqrt} function from numpy and the \texttt{mse} function from sklearn. Initial training was promising, with a RMSE of just 0.0753 for the validation set. To further reduce this value, hyper-parameter optimisation was once again employed using Optuna. 

\newpage
The hyper-parameter search space was defined according to~\ref{sec:gbm-training}, and an Optuna study was set up and run. The results of the Optuna study are shown below:

	% -------------------------------------------------------------
	% OPTUNA TRIAL HISTORY
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.27]{optuna-trial-history}
	\caption{RMSE with each trial (hyper-parameter configuration)}
	\label{default}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% OPTUNA TRIAL HISTORY
	% -------------------------------------------------------------

Because of the computational efficiency of LightGBM and the relatively speedy process of training a GBM model, 50 different combinations were explored in a short time period. Additionally the use of early stopping (where training is cut short if a model doesn't improve after a certain number of steps) after 10 steps was used to speed up training. The Optuna study was run and each trial recorded, with the best RMSE value shown as 0.07389 and best configuration identified as: \{'num\_leaves': 67, 'learning\_rate': 0.08101, 'min\_data\_in\_leaf': 110, 'max\_depth': 7\}. These hyper-parameters were saved into a 'best\_params' variable so they could be used later in the pipeline to train the final models. 

Additionally, the Optuna visualisation tools were used to plot a visualisation of how RMSE was reduced progressively as the search space was explored.

\subsection[Evaluation of Sentiment and Non-Sentiment Based Models]{Evaluation of Sentiment and Non-Sentiment Based Models}
\label{section:xpm-eval}
To evaluate how sentiment data affected the model, it made sense to train another model using the same dataset minus the \texttt{'positive\_tweets'} and\\
 \texttt{'negative\_tweets'} features. 

This set up allowed for a fair comparison between the two models, and any notable differences could be observed. To create both models, new dataframes for the training and evaluation data were created by taking copies of the originals and dropping the sentiment data columns. Two model objects were then created and trained on the data using the hyper-parameter configuration obtained from the Optuna study. 

Initially the RMSE was calculated manually like for the initial model and then compared between the two models. By comparing the RMSE for the same target variables in both models, we can get an idea of how far each model's predictions are from the true values. After examining the difference between the RMSE of the two models, it was found the difference was fairly small and likely within the margin of error. To combat this, cross validation was performed using the \texttt{cross\_val\_score} method from sklearn with a \texttt{cv} value of 5. This method splits the data into 5 training and test sets, trains a model on each split, and then averages the RMSE across all splits. Using cross-validation ensures that any observed differences between models are not due to random variability in specific train-test splits, but more general differences in model performance.

To visualise the differences between the predictions of each model, a histogram of residuals (true values - predicted values) was plotted using matplotlib. As well as this, a feature importance chart was created using lightGBM's \texttt{plot\_importance} method.

These visualisations, along with RMSE comparisons are detailed in section~\ref{section:model-comparisons}.
% -------------------------------------------------------------
% TEAM SELECTION ALGORITHM
% -------------------------------------------------------------
\section[Team Selection Algorithm]{Team Selection Algorithm}
The team selection algorithm was also implemented using Python in Google Colab. Python was chosen because of the PuLP library that can be used to solve linear integer programming problems, and Colab was used for consistency with the rest of the project, along with the added readability that markup text provides.

PuLP allowed for the easy definition of the problem and constraints, with a simple API for solving the problem with just a few lines of code. The Colab environment allowed for quick experimentation and visualisation of results, and its familiar format used for the other components of the prediction model meant it was an ideal platform for the development of the team selection pipeline.
\subsection[Squad Selection]{Squad Selection}
The first stage of the team selection pipeline was to define a function that returned an optimal squad of 15 players based on some points predictions. 

The problem was defined by creating an object from the \texttt{LpProblem} class in PuLP, with the aim set to \texttt{pulp.LpMaximize}, meaning the goal is to maximise the total number of predicted points. For each player in the dataset, a binary decision variable was created using the \texttt{LpVariable} class. This variable is set to 1 if the player is selected for the team, and 0 otherwise.

The objective function was then added to the problem. It calculates the total predicted points of all selected players by multiplying each player's predicted points by their decision variable, and summing the results. This was done using the \texttt{lpSum} function which tells the solver to pick a set of players such that the total expected points is as high as possible.

Next the constraints were added to make sure the selected squad follows the FPL rules. Each constraint was added using \texttt{lpSum} to ensure constraints were satisfied based on the player data in the input DataFrame. After defining the full optimisation problem, \texttt{problem.solve()} was used to compute the optimal squad. The final selected players were extracted by checking which decision variables were set to 1. Finally a dataframe containing the name, team, position, value, expected points, and actual points for all 15 selected players was returned.

\subsection[Team Selection and Real-World Performance Evaulation]{Team Selection and Real-World Performance Evaulation}
The second stage of the selection pipeline was to use the squad selection function to generate squads for each model and each gameweek. Then a starting eleven team was chosen from each squad, a captaincy chip was applied and the final points total was calculated so each model could be evaluated.

For selection of a starting eleven a function was defined that took the squad of 15 as input, then sorted players into dataframes for each position ordered by expected points. An initial team of 8 was constructed by using the \texttt{.head()} method for each position's dataframe: 

\begin{verbatim} starting = pd.concat([
        gks.head(1),
        defs.head(3),
        mids.head(3),
        fwds.head(1)
    ]) \end{verbatim}

To fill the rest of the available positions, the remaining players were combined into one dataframe again sorted by expected points, and the top three players were selected to form a team of eleven.
 
With both functions defined, the teams and resulting points for each model and each gameweek could be generated. To do this a dictionary was created with arrays for the \texttt{all\_data}, \texttt{no\_sentiment\_data}, and \texttt{optimal\_score} models. Then the gameweeks were looped through, extracting the relevant columns for expected points for each model and the true points scored, this information was added to the arrays in the dictionary.
 
Then for each gameweek and model, the \texttt{select\_fpl\_team} function defined earlier was called with the relevant data to get the optimal squad. This was run through the \texttt{select\_starting\_xi} function to get the best eleven players, and then the actual points scored by that eleven was calculated using \texttt{.sum()}. The best player (player with highest expected points) from the eleven had their points added to the total again to simulate their points being doubled.
 
The resulting teams and points scored for each week were saved for evaluation, with visualisations being generated using matplotlib. The results of final evaluation are detailed in chapter~\ref{chapter:results}.
% -------------------------------------------------------------
% RESULTS AND DISCUSSION
% -------------------------------------------------------------
\chapter[Results and Discussion]{Results and Discussion} 
\label{chapter:results}
The goal of this project is to evaluate the performance of a new FPL prediction model constructed using existing methods, but with the inclusion of a novel sentiment data source - Twitter. As such, this chapter outlines the findings of final model evaluation over multiple gameweeks alongside existing baselines.

The model is compared against the baseline average scores of all managers, a Gradient Boosting Machine model without twitter data, the final GBM model that includes twitter data, and the best possible score that could have been achieved. The results suggest that the lack of Twitter data available currently prevent any solid conclusions from being drawn about how effective it is as a sentiment data source. As well as this the limited timescale from which to draw comparisons (just 8 gameweeks) further limit the extent to which any concrete conclusions can be made. The GBM models constructed both perform almost identically with minimal differences, outscoring the average player consistently whilst remaining far away from the best possible score.

\section[Model Performance Evaluation]{Model Performance Evaluation}
\label{section:model-comparisons}
As outlined in section~\ref{section:xpm-eval}, Root Mean Squared Error (RMSE) was used to evaluate the performance of the expected points model. RMSE measures the average magnitude of prediction errors, where lower values indicate better performance. Additionally, to evaluate how much impact the sentiment data had on the model, another model was trained using a modified dataset that excludes all twitter sentiment information, and the RSME of the two models compared. By comparing RMSE for the same GBM model and hyper-parameter configuration, we can observe if the model trained using sentiment information generates predictions closer to the true values.

Cross validation was used to ensure differences in RMSE for each model were not due to randomness or a favourable train test split. The results showed that the average RMSE for the model trained with sentiment data was \textbf{0.0711 ± 0.0060}, while the model without sentiment data achieved an average RMSE of \textbf{0.0711 ± 0.0059}. Although the RMSE for the sentiment model was marginally lower, the difference falls well within the standard deviation. This seems to suggest that sentiment data did not significantly improve model performance in terms of RMSE alone.

To further explore the differences between models, and to ensure there is no significant difference that was not captured by RMSE alone, additional analysis was performed. This included a plot of the residual distribution between both models (\ref{residual-histogram}), as well as a feature importance chart (\ref{feature-importance}).


	% -------------------------------------------------------------
	% RESIDUAL DISTRIBUTION
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.32]{residuals-distribution}
	\caption{Distribution of residuals for each model}
	\label{residual-histogram}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% RESIDUAL DISTRIBUTION
	% -------------------------------------------------------------
	
	% -------------------------------------------------------------
	% FEATURE IMPORTANCE
	% -------------------------------------------------------------
	\begin{figure}[htbp]
	\begin{center}
	\includegraphics[scale=0.43]{feature-importance}
	\caption{Feature importance}
	\label{feature-importance}
	\end{center}
	\end{figure}
	% -------------------------------------------------------------
	% FEATURE IMPORTANCE
	% -------------------------------------------------------------
% -------------------------------------------------------------
% CONCLUSION AND FUTURE WORK
% -------------------------------------------------------------
\chapter[Conclusion and Future Work]{Conclusion and Future Work}
% -------------------------------------------------------------
% BIBLIOGRAPHY
% -------------------------------------------------------------
\begingroup
\raggedright
\bibliographystyle{IEEEtran}
\bibliography{Bib}
\endgroup




\end{document}